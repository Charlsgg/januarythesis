{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0112f9e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading and cleaning data...\n",
      "Training Data Size after injection: 34088\n",
      "\n",
      "Initializing BERT with high Dropout (to prevent overfitting)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\charls\\AppData\\Local\\Temp\\ipykernel_21444\\4213858408.py:165: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = torch.cuda.amp.GradScaler()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/7670 [00:00<?, ?it/s]C:\\Users\\charls\\AppData\\Local\\Temp\\ipykernel_21444\\4213858408.py:182: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast():\n",
      "Epoch 1:   2%|â–         | 127/7670 [00:10<10:25, 12.06it/s, loss=0.14]  \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 186\u001b[0m\n\u001b[0;32m    183\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m model(input_ids, attention_mask\u001b[38;5;241m=\u001b[39mmask, labels\u001b[38;5;241m=\u001b[39mlabels)\n\u001b[0;32m    184\u001b[0m     loss \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mloss \u001b[38;5;241m/\u001b[39m ACCUMULATION_STEPS \u001b[38;5;66;03m# Normalize loss\u001b[39;00m\n\u001b[1;32m--> 186\u001b[0m \u001b[43mscaler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscale\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    188\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (i \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m%\u001b[39m ACCUMULATION_STEPS \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    189\u001b[0m     scaler\u001b[38;5;241m.\u001b[39mstep(optimizer)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_tensor.py:581\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    571\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    572\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    573\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    574\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    579\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    580\u001b[0m     )\n\u001b[1;32m--> 581\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    582\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    583\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\autograd\\__init__.py:347\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    342\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    344\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    345\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    346\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 347\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    348\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    349\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    350\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    351\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    352\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    353\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    354\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    355\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\autograd\\graph.py:825\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[1;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    823\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[0;32m    824\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 825\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    826\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[0;32m    827\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    828\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    829\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, BertConfig\n",
    "from torch.optim import AdamW\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import classification_report\n",
    "import re\n",
    "import random\n",
    "import gc\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "# --- CONFIGURATION ---\n",
    "INPUT_FILE = \"robust_experience_training_data_unbalanced.csv\" # Your original source\n",
    "MAX_LEN = 128\n",
    "BATCH_SIZE = 4\n",
    "ACCUMULATION_STEPS = 8  # Effective Batch Size = 32 (4 * 8) -> Much more stable\n",
    "EPOCHS = 6              # Lower epochs to prevent overfitting\n",
    "LR = 2e-5\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# --- 1. ROBUST TEXT CLEANING ---\n",
    "def clean_text(text):\n",
    "    text = str(text).lower()\n",
    "    # Remove dates (e.g., Jan 2020, 2019-2022)\n",
    "    text = re.sub(r'\\b(jan|feb|mar|apr|may|jun|jul|aug|sep|oct|nov|dec)[a-z]*\\.? \\d{4}', '', text)\n",
    "    text = re.sub(r'\\d{4}\\s?-\\s?(present|current|\\d{4})', '', text)\n",
    "    # Remove locations (simple heuristic for common resume formats)\n",
    "    text = re.sub(r'[a-z]+, [a-z]{2} \\|', '', text) \n",
    "    # Remove extra spaces\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text\n",
    "\n",
    "# --- 2. GENERATE INTELLIGENT ADVERSARIAL DATA ---\n",
    "# This creates specific \"Hard Negatives\" to force the model to learn context.\n",
    "def generate_adversarial_data():\n",
    "    data = []\n",
    "\n",
    "    # A. TEACH: \"Consultant\" can be Full-time\n",
    "    ft_consultant_templates = [\n",
    "        \"Full-time Implementation Consultant at Oracle. I work permanently for the firm.\",\n",
    "        \"Senior Strategy Consultant (Internal Employee). Not a contractor.\",\n",
    "        \"Staff Consultant. Led the client project as a full-time employee.\",\n",
    "        \"Permanent Solutions Consultant. Salaried role with benefits.\",\n",
    "        \"Principal Consultant. I was employed full-time to manage key accounts.\"\n",
    "    ]\n",
    "    # Replicate these heavily to force the model to learn\n",
    "    for _ in range(200): \n",
    "        data.append({'text': random.choice(ft_consultant_templates), 'label': 'Full-time'})\n",
    "\n",
    "    # B. TEACH: \"Managing Interns\" is Full-time, not Internship\n",
    "    ft_manager_templates = [\n",
    "        \"Engineering Lead. My responsibilities included mentoring summer interns.\",\n",
    "        \"Senior Manager. I managed a team of 3 interns and 2 juniors.\",\n",
    "        \"Program Coordinator. I organized the internship orientation.\",\n",
    "        \"Director of Operations. Oversaw the summer analyst internship program.\",\n",
    "        \"Team Lead. Conducted code reviews for the intern team.\"\n",
    "    ]\n",
    "    for _ in range(200):\n",
    "        data.append({'text': random.choice(ft_manager_templates), 'label': 'Full-time'})\n",
    "\n",
    "    # C. TEACH: \"Intern\" means you are learning, not working full-time\n",
    "    intern_templates = [\n",
    "        \"Marketing Intern. Assisted the manager with social media.\",\n",
    "        \"Summer Analyst. Rotational program in finance and risk.\",\n",
    "        \"Engineering Intern. Shadowed senior developers and fixed bugs.\",\n",
    "        \"Co-op Student. 4-month placement during university.\",\n",
    "        \"Virtual Intern. Completed simulated tasks for the company.\"\n",
    "    ]\n",
    "    for _ in range(200):\n",
    "        data.append({'text': random.choice(intern_templates), 'label': 'Internship'})\n",
    "\n",
    "    # D. TEACH: \"Contractor/Freelance\" vs Full-time\n",
    "    freelance_templates = [\n",
    "        \"Independent Contractor. Worked on a per-project basis.\",\n",
    "        \"Freelance Consultant. Advised multiple clients simultaneously.\",\n",
    "        \"Self-employed Developer. Built websites for local businesses.\",\n",
    "        \"Gig Worker. Completed tasks via a mobile platform.\",\n",
    "        \"Temporary Staff. 3-month contract to cover seasonal demand.\"\n",
    "    ]\n",
    "    for _ in range(200):\n",
    "        data.append({'text': random.choice(freelance_templates), 'label': 'Freelance'})\n",
    "\n",
    "    return pd.DataFrame(data)\n",
    "\n",
    "# --- 3. LOAD AND PREPARE DATA ---\n",
    "if not os.path.exists(INPUT_FILE):\n",
    "    print(f\"Error: {INPUT_FILE} not found. Please ensure your base CSV is present.\")\n",
    "else:\n",
    "    print(\"Loading and cleaning data...\")\n",
    "    df = pd.read_csv(INPUT_FILE)\n",
    "    \n",
    "    # Clean original data\n",
    "    df['text'] = df['text'].apply(clean_text)\n",
    "    \n",
    "    # Generate and append the \"Smart Poison\"\n",
    "    df_poison = generate_adversarial_data()\n",
    "    df = pd.concat([df, df_poison], ignore_index=True)\n",
    "    \n",
    "    # Shuffle\n",
    "    df = df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "    print(f\"Training Data Size after injection: {len(df)}\")\n",
    "\n",
    "    # Encode Labels\n",
    "    le = LabelEncoder()\n",
    "    df['label_encoded'] = le.fit_transform(df['label'])\n",
    "    classes = le.classes_\n",
    "\n",
    "    # Split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        df['text'].values, df['label_encoded'].values, test_size=0.1, random_state=42, stratify=df['label_encoded']\n",
    "    )\n",
    "\n",
    "    # --- 4. DATASET CLASS ---\n",
    "    class ResumeDataset(Dataset):\n",
    "        def __init__(self, texts, labels, tokenizer, max_len):\n",
    "            self.texts = texts\n",
    "            self.labels = labels\n",
    "            self.tokenizer = tokenizer\n",
    "            self.max_len = max_len\n",
    "\n",
    "        def __len__(self):\n",
    "            return len(self.texts)\n",
    "\n",
    "        def __getitem__(self, item):\n",
    "            text = str(self.texts[item])\n",
    "            label = self.labels[item]\n",
    "            encoding = self.tokenizer.encode_plus(\n",
    "                text, add_special_tokens=True, max_length=self.max_len,\n",
    "                return_token_type_ids=False, padding='max_length',\n",
    "                truncation=True, return_attention_mask=True, return_tensors='pt',\n",
    "            )\n",
    "            return {\n",
    "                'input_ids': encoding['input_ids'].flatten(),\n",
    "                'attention_mask': encoding['attention_mask'].flatten(),\n",
    "                'labels': torch.tensor(label, dtype=torch.long)\n",
    "            }\n",
    "\n",
    "    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "    train_dataset = ResumeDataset(X_train, y_train, tokenizer, MAX_LEN)\n",
    "    test_dataset = ResumeDataset(X_test, y_test, tokenizer, MAX_LEN)\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE)\n",
    "\n",
    "    # --- 5. ROBUST MODEL SETUP ---\n",
    "    print(\"\\nInitializing BERT with high Dropout (to prevent overfitting)...\")\n",
    "    config = BertConfig.from_pretrained(\n",
    "        'bert-base-uncased', \n",
    "        num_labels=len(classes),\n",
    "        hidden_dropout_prob=0.3,          # Increased from 0.1\n",
    "        attention_probs_dropout_prob=0.3  # Increased from 0.1\n",
    "    )\n",
    "    model = BertForSequenceClassification.from_pretrained('bert-base-uncased', config=config)\n",
    "    model = model.to(DEVICE)\n",
    "    \n",
    "    optimizer = AdamW(model.parameters(), lr=LR)\n",
    "    \n",
    "    # GradScaler for FP16\n",
    "    scaler = torch.cuda.amp.GradScaler()\n",
    "\n",
    "    # --- 6. TRAINING LOOP WITH ACCUMULATION ---\n",
    "    print(\"\\nStarting Training...\")\n",
    "    \n",
    "    for epoch in range(EPOCHS):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        loop = tqdm(train_loader, leave=True)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        for i, batch in enumerate(loop):\n",
    "            input_ids = batch['input_ids'].to(DEVICE)\n",
    "            mask = batch['attention_mask'].to(DEVICE)\n",
    "            labels = batch['labels'].to(DEVICE)\n",
    "\n",
    "            with torch.cuda.amp.autocast():\n",
    "                outputs = model(input_ids, attention_mask=mask, labels=labels)\n",
    "                loss = outputs.loss / ACCUMULATION_STEPS # Normalize loss\n",
    "\n",
    "            scaler.scale(loss).backward()\n",
    "\n",
    "            if (i + 1) % ACCUMULATION_STEPS == 0:\n",
    "                scaler.step(optimizer)\n",
    "                scaler.update()\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "            total_loss += loss.item() * ACCUMULATION_STEPS\n",
    "            loop.set_description(f\"Epoch {epoch+1}\")\n",
    "            loop.set_postfix(loss=loss.item() * ACCUMULATION_STEPS)\n",
    "\n",
    "        print(f\"Avg Loss: {total_loss / len(train_loader)}\")\n",
    "\n",
    "    # --- 7. EVALUATION ON TRICKY CASES ---\n",
    "    print(\"\\nEvaluating on Tricky Adversarial Cases...\")\n",
    "    model.eval()\n",
    "\n",
    "    def predict(text):\n",
    "        text = clean_text(text) # Clean input before predicting\n",
    "        encoding = tokenizer.encode_plus(\n",
    "            text, add_special_tokens=True, max_length=MAX_LEN,\n",
    "            return_token_type_ids=False, padding='max_length',\n",
    "            truncation=True, return_attention_mask=True, return_tensors='pt',\n",
    "        )\n",
    "        input_ids = encoding['input_ids'].to(DEVICE)\n",
    "        mask = encoding['attention_mask'].to(DEVICE)\n",
    "        with torch.no_grad():\n",
    "            outputs = model(input_ids, attention_mask=mask)\n",
    "            probs = torch.nn.functional.softmax(outputs.logits, dim=1)\n",
    "        conf, idx = torch.max(probs, dim=1)\n",
    "        return classes[idx.item()], conf.item()\n",
    "\n",
    "    test_cases = [\n",
    "        \"Marketing Intern assisting with social media scheduling.\", # Should be Internship\n",
    "        \"Virtual Intern: Participated in a 4-week remote simulation.\", # Should be Internship\n",
    "        \"Engineering Lead responsible for mentoring 3 summer interns.\", # Should be Full-time\n",
    "        \"Full-time Implementation Consultant working exclusively for Oracle.\", # Should be Full-time\n",
    "        \"Internal Strategy Consultant employed permanently by the firm.\", # Should be Full-time\n",
    "        \"Freelance Web Developer building WordPress sites.\", # Should be Freelance\n",
    "        \"Temporary Staff handling seasonal overflow.\", # Should be Freelance (or Full-time depending on definition, but usually Freelance/Contract)\n",
    "        \"Acting Interim Lead for the design team during a summer placement.\" # Should be Full-time\n",
    "    ]\n",
    "\n",
    "    print(f\"{'PREDICTION':<15} | {'CONF.':<8} | {'TEXT'}\")\n",
    "    print(\"=\"*80)\n",
    "    for text in test_cases:\n",
    "        pred, conf = predict(text)\n",
    "        color = \"ðŸŸ¢\" if conf > 0.8 else \"ðŸ”´\"\n",
    "        print(f\"{pred.upper():<15} | {conf*100:.1f}% {color} | {text}\")\n",
    "\n",
    "    # Save\n",
    "    SAVE_PATH = \"./robust_bert_model\"\n",
    "    if not os.path.exists(SAVE_PATH): os.makedirs(SAVE_PATH)\n",
    "    model.save_pretrained(SAVE_PATH)\n",
    "    tokenizer.save_pretrained(SAVE_PATH)\n",
    "    np.save(os.path.join(SAVE_PATH, 'classes.npy'), classes)\n",
    "    print(f\"\\nModel saved to {SAVE_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d889f53d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on: cuda\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, BertConfig\n",
    "from torch.optim import AdamW\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import classification_report\n",
    "import re\n",
    "import random\n",
    "import gc\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "# --- CONFIGURATION ---\n",
    "INPUT_FILE = \"robust_experience_training_data_unbalanced.csv\"\n",
    "MAX_LEN = 128\n",
    "BATCH_SIZE = 4\n",
    "ACCUMULATION_STEPS = 8  # Effective Batch Size = 32\n",
    "EPOCHS = 6              \n",
    "LR = 2e-5\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "print(f\"Running on: {DEVICE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d650372b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- ROBUST TEXT CLEANING ---\n",
    "def clean_text(text):\n",
    "    text = str(text).lower()\n",
    "    # Remove dates (e.g., Jan 2020, 2019-2022) to prevent date-overfitting\n",
    "    text = re.sub(r'\\b(jan|feb|mar|apr|may|jun|jul|aug|sep|oct|nov|dec)[a-z]*\\.? \\d{4}', '', text)\n",
    "    text = re.sub(r'\\d{4}\\s?-\\s?(present|current|\\d{4})', '', text)\n",
    "    # Remove locations \n",
    "    text = re.sub(r'[a-z]+, [a-z]{2} \\|', '', text) \n",
    "    # Remove extra spaces\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text\n",
    "\n",
    "# --- GENERATE INTELLIGENT ADVERSARIAL DATA ---\n",
    "def generate_adversarial_data():\n",
    "    data = []\n",
    "\n",
    "    # A. TEACH: \"Consultant\" can be Full-time\n",
    "    ft_consultant_templates = [\n",
    "        \"Full-time Implementation Consultant at Oracle. I work permanently for the firm.\",\n",
    "        \"Senior Strategy Consultant (Internal Employee). Not a contractor.\",\n",
    "        \"Staff Consultant. Led the client project as a full-time employee.\",\n",
    "        \"Permanent Solutions Consultant. Salaried role with benefits.\",\n",
    "        \"Principal Consultant. I was employed full-time to manage key accounts.\"\n",
    "    ]\n",
    "    for _ in range(200): \n",
    "        data.append({'text': random.choice(ft_consultant_templates), 'label': 'Full-time'})\n",
    "\n",
    "    # B. TEACH: \"Managing Interns\" is Full-time, not Internship\n",
    "    ft_manager_templates = [\n",
    "        \"Engineering Lead. My responsibilities included mentoring summer interns.\",\n",
    "        \"Senior Manager. I managed a team of 3 interns and 2 juniors.\",\n",
    "        \"Program Coordinator. I organized the internship orientation.\",\n",
    "        \"Director of Operations. Oversaw the summer analyst internship program.\",\n",
    "        \"Team Lead. Conducted code reviews for the intern team.\"\n",
    "    ]\n",
    "    for _ in range(200):\n",
    "        data.append({'text': random.choice(ft_manager_templates), 'label': 'Full-time'})\n",
    "\n",
    "    # C. TEACH: \"Intern\" means you are learning, not working full-time\n",
    "    intern_templates = [\n",
    "        \"Marketing Intern. Assisted the manager with social media.\",\n",
    "        \"Summer Analyst. Rotational program in finance and risk.\",\n",
    "        \"Engineering Intern. Shadowed senior developers and fixed bugs.\",\n",
    "        \"Co-op Student. 4-month placement during university.\",\n",
    "        \"Virtual Intern. Completed simulated tasks for the company.\"\n",
    "    ]\n",
    "    for _ in range(200):\n",
    "        data.append({'text': random.choice(intern_templates), 'label': 'Internship'})\n",
    "\n",
    "    # D. TEACH: \"Contractor/Freelance\" vs Full-time\n",
    "    freelance_templates = [\n",
    "        \"Independent Contractor. Worked on a per-project basis.\",\n",
    "        \"Freelance Consultant. Advised multiple clients simultaneously.\",\n",
    "        \"Self-employed Developer. Built websites for local businesses.\",\n",
    "        \"Gig Worker. Completed tasks via a mobile platform.\",\n",
    "        \"Temporary Staff. 3-month contract to cover seasonal demand.\"\n",
    "    ]\n",
    "    for _ in range(200):\n",
    "        data.append({'text': random.choice(freelance_templates), 'label': 'Freelance'})\n",
    "\n",
    "    return pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c2e61404",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading and cleaning data...\n",
      "Training Data Size after injection: 34088\n",
      "Data prepared successfully.\n"
     ]
    }
   ],
   "source": [
    "if not os.path.exists(INPUT_FILE):\n",
    "    print(f\"Error: {INPUT_FILE} not found. Please ensure your base CSV is present.\")\n",
    "else:\n",
    "    print(\"Loading and cleaning data...\")\n",
    "    df = pd.read_csv(INPUT_FILE)\n",
    "    \n",
    "    # 1. Clean original data\n",
    "    df['text'] = df['text'].apply(clean_text)\n",
    "    \n",
    "    # 2. Inject \"Smart Poison\"\n",
    "    df_poison = generate_adversarial_data()\n",
    "    df = pd.concat([df, df_poison], ignore_index=True)\n",
    "    \n",
    "    # 3. Shuffle\n",
    "    df = df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "    print(f\"Training Data Size after injection: {len(df)}\")\n",
    "\n",
    "    # 4. Encode Labels\n",
    "    le = LabelEncoder()\n",
    "    df['label_encoded'] = le.fit_transform(df['label'])\n",
    "    classes = le.classes_\n",
    "\n",
    "    # 5. Split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        df['text'].values, df['label_encoded'].values, test_size=0.1, random_state=42, stratify=df['label_encoded']\n",
    "    )\n",
    "    \n",
    "    print(\"Data prepared successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "69a370d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading and cleaning data...\n",
      "Training Data Size after injection: 34088\n",
      "Data prepared successfully.\n"
     ]
    }
   ],
   "source": [
    "if not os.path.exists(INPUT_FILE):\n",
    "    print(f\"Error: {INPUT_FILE} not found. Please ensure your base CSV is present.\")\n",
    "else:\n",
    "    print(\"Loading and cleaning data...\")\n",
    "    df = pd.read_csv(INPUT_FILE)\n",
    "    \n",
    "    # 1. Clean original data\n",
    "    df['text'] = df['text'].apply(clean_text)\n",
    "    \n",
    "    # 2. Inject \"Smart Poison\"\n",
    "    df_poison = generate_adversarial_data()\n",
    "    df = pd.concat([df, df_poison], ignore_index=True)\n",
    "    \n",
    "    # 3. Shuffle\n",
    "    df = df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "    print(f\"Training Data Size after injection: {len(df)}\")\n",
    "\n",
    "    # 4. Encode Labels\n",
    "    le = LabelEncoder()\n",
    "    df['label_encoded'] = le.fit_transform(df['label'])\n",
    "    classes = le.classes_\n",
    "\n",
    "    # 5. Split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        df['text'].values, df['label_encoded'].values, test_size=0.1, random_state=42, stratify=df['label_encoded']\n",
    "    )\n",
    "    \n",
    "    print(\"Data prepared successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "232387aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataloaders ready.\n"
     ]
    }
   ],
   "source": [
    "class ResumeDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_len):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        text = str(self.texts[item])\n",
    "        label = self.labels[item]\n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            text, add_special_tokens=True, max_length=self.max_len,\n",
    "            return_token_type_ids=False, padding='max_length',\n",
    "            truncation=True, return_attention_mask=True, return_tensors='pt',\n",
    "        )\n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'labels': torch.tensor(label, dtype=torch.long)\n",
    "        }\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "train_dataset = ResumeDataset(X_train, y_train, tokenizer, MAX_LEN)\n",
    "test_dataset = ResumeDataset(X_test, y_test, tokenizer, MAX_LEN)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE)\n",
    "print(\"Dataloaders ready.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45d4fd85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing BERT with high Dropout...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model initialized.\n"
     ]
    }
   ],
   "source": [
    "print(\"Initializing BERT with high Dropout...\")\n",
    "config = BertConfig.from_pretrained(\n",
    "    'bert-base-uncased', \n",
    "    num_labels=len(classes),\n",
    "    hidden_dropout_prob=0.3,          # High dropout to prevent overfitting\n",
    "    attention_probs_dropout_prob=0.3  \n",
    ")\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', config=config)\n",
    "model = model.to(DEVICE)\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=LR)\n",
    "# scaler = torch.cuda.amp.GradScaler() # For mixed precision training\n",
    "# Old way (what you have):\n",
    "# scaler = torch.cuda.amp.GradScaler()\n",
    "\n",
    "# New way (removes warning):\n",
    "scaler = torch.amp.GradScaler('cuda')\n",
    "\n",
    "print(\"Model initialized.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cac387cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/7670 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\charls\\AppData\\Local\\Temp\\ipykernel_21444\\1318387829.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast():\n",
      "Epoch 1:  14%|â–ˆâ–        | 1069/7670 [10:25<1:04:20,  1.71it/s, loss=0.0207]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 26\u001b[0m\n\u001b[0;32m     23\u001b[0m     scaler\u001b[38;5;241m.\u001b[39mupdate()\n\u001b[0;32m     24\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m---> 26\u001b[0m total_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m*\u001b[39m ACCUMULATION_STEPS\n\u001b[0;32m     27\u001b[0m loop\u001b[38;5;241m.\u001b[39mset_description(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     28\u001b[0m loop\u001b[38;5;241m.\u001b[39mset_postfix(loss\u001b[38;5;241m=\u001b[39mloss\u001b[38;5;241m.\u001b[39mitem() \u001b[38;5;241m*\u001b[39m ACCUMULATION_STEPS)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "print(\"Starting Training...\")\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    loop = tqdm(train_loader, leave=True)\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    for i, batch in enumerate(loop):\n",
    "        input_ids = batch['input_ids'].to(DEVICE)\n",
    "        mask = batch['attention_mask'].to(DEVICE)\n",
    "        labels = batch['labels'].to(DEVICE)\n",
    "\n",
    "        with torch.cuda.amp.autocast():\n",
    "            outputs = model(input_ids, attention_mask=mask, labels=labels)\n",
    "            loss = outputs.loss / ACCUMULATION_STEPS # Normalize loss\n",
    "\n",
    "        scaler.scale(loss).backward()\n",
    "\n",
    "        if (i + 1) % ACCUMULATION_STEPS == 0:\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "        total_loss += loss.item() * ACCUMULATION_STEPS\n",
    "        loop.set_description(f\"Epoch {epoch+1}\")\n",
    "        loop.set_postfix(loss=loss.item() * ACCUMULATION_STEPS)\n",
    "\n",
    "    print(f\"Avg Loss: {total_loss / len(train_loader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f08ea93",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nEvaluating on Tricky Adversarial Cases...\")\n",
    "model.eval()\n",
    "\n",
    "def predict(text):\n",
    "    text = clean_text(text) # Use the same cleaning function\n",
    "    encoding = tokenizer.encode_plus(\n",
    "        text, add_special_tokens=True, max_length=MAX_LEN,\n",
    "        return_token_type_ids=False, padding='max_length',\n",
    "        truncation=True, return_attention_mask=True, return_tensors='pt',\n",
    "    )\n",
    "    input_ids = encoding['input_ids'].to(DEVICE)\n",
    "    mask = encoding['attention_mask'].to(DEVICE)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids, attention_mask=mask)\n",
    "        probs = torch.nn.functional.softmax(outputs.logits, dim=1)\n",
    "    conf, idx = torch.max(probs, dim=1)\n",
    "    return classes[idx.item()], conf.item()\n",
    "\n",
    "test_cases = [\n",
    "    \"Marketing Intern assisting with social media scheduling.\", \n",
    "    \"Virtual Intern: Participated in a 4-week remote simulation.\", \n",
    "    \"Engineering Lead responsible for mentoring 3 summer interns.\", \n",
    "    \"Full-time Implementation Consultant working exclusively for Oracle.\", \n",
    "    \"Internal Strategy Consultant employed permanently by the firm.\", \n",
    "    \"Freelance Web Developer building WordPress sites.\", \n",
    "    \"Temporary Staff handling seasonal overflow.\", \n",
    "    \"Acting Interim Lead for the design team during a summer placement.\" \n",
    "]\n",
    "\n",
    "print(f\"{'PREDICTION':<15} | {'CONF.':<8} | {'TEXT'}\")\n",
    "print(\"=\"*80)\n",
    "for text in test_cases:\n",
    "    pred, conf = predict(text)\n",
    "    color = \"ðŸŸ¢\" if conf > 0.8 else \"ðŸ”´\"\n",
    "    print(f\"{pred.upper():<15} | {conf*100:.1f}% {color} | {text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c763f28",
   "metadata": {},
   "outputs": [],
   "source": [
    "SAVE_PATH = \"./robust_bert_model\"\n",
    "if not os.path.exists(SAVE_PATH): os.makedirs(SAVE_PATH)\n",
    "model.save_pretrained(SAVE_PATH)\n",
    "tokenizer.save_pretrained(SAVE_PATH)\n",
    "np.save(os.path.join(SAVE_PATH, 'classes.npy'), classes)\n",
    "print(f\"\\nModel saved to {SAVE_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c94fc95",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1841848",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "785b9bfc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
