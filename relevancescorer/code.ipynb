{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d411d5b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resume 1 Relevance: 0.8349\n",
      "Resume 2 Relevance: 0.6166\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "class BertRelevanceScorer:\n",
    "    def __init__(self):\n",
    "        # Load pre-trained BERT model and tokenizer\n",
    "        # We use 'bert-base-uncased' as requested\n",
    "        self.model_name = 'bert-base-uncased'\n",
    "        self.tokenizer = BertTokenizer.from_pretrained(self.model_name)\n",
    "        self.model = BertModel.from_pretrained(self.model_name)\n",
    "        \n",
    "        # Set to evaluation mode (saves memory/speed)\n",
    "        self.model.eval()\n",
    "\n",
    "    def get_embedding(self, text):\n",
    "        \"\"\"\n",
    "        Converts text into a single vector using Mean Pooling.\n",
    "        \"\"\"\n",
    "        # 1. Tokenize\n",
    "        inputs = self.tokenizer(text, return_tensors='pt', truncation=True, padding=True, max_length=512)\n",
    "        \n",
    "        # 2. Pass through BERT model\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model(**inputs)\n",
    "            \n",
    "        # 3. Get the \"Last Hidden State\" (Vectors for every word)\n",
    "        # Shape: [batch_size, seq_len, hidden_dim]\n",
    "        token_embeddings = outputs.last_hidden_state\n",
    "        \n",
    "        # 4. Get Attention Mask (to ignore padding tokens)\n",
    "        attention_mask = inputs['attention_mask']\n",
    "\n",
    "        # 5. MEAN POOLING (The critical step)\n",
    "        # We average the vectors, but only for real words (not padding)\n",
    "        \n",
    "        # Expand mask to match embedding size\n",
    "        mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "        \n",
    "        # Sum of vectors / Sum of valid tokens\n",
    "        sum_embeddings = torch.sum(token_embeddings * mask_expanded, 1)\n",
    "        sum_mask = torch.clamp(mask_expanded.sum(1), min=1e-9)\n",
    "        \n",
    "        mean_pooled_embedding = sum_embeddings / sum_mask\n",
    "        \n",
    "        return mean_pooled_embedding\n",
    "\n",
    "    def calculate_relevance(self, resume_text, job_description):\n",
    "        \"\"\"\n",
    "        Calculates Cosine Similarity (0 to 1) between Resume and Job.\n",
    "        \"\"\"\n",
    "        # Get vectors\n",
    "        resume_vec = self.get_embedding(resume_text)\n",
    "        job_vec = self.get_embedding(job_description)\n",
    "        \n",
    "        # Calculate Cosine Similarity\n",
    "        # transform to numpy for sklearn\n",
    "        similarity = cosine_similarity(resume_vec.numpy(), job_vec.numpy())\n",
    "        \n",
    "        # Return as a float (0.0 to 1.0)\n",
    "        return float(similarity[0][0])\n",
    "\n",
    "# --- Example Usage ---\n",
    "if __name__ == \"__main__\":\n",
    "    scorer = BertRelevanceScorer()\n",
    "    \n",
    "    job_desc = \"Looking for a software engineer with Python developer and Machine Learning experience.\"\n",
    "    \n",
    "    resume_1 = \"I am a Python developer with 3 years of experience in AI and deep learning.\"\n",
    "    resume_2 = \"I am a graphic designer skilled in Photoshop and Illustrator.\"\n",
    "    \n",
    "    score_1 = scorer.calculate_relevance(resume_1, job_desc)\n",
    "    score_2 = scorer.calculate_relevance(resume_2, job_desc)\n",
    "    \n",
    "    print(f\"Resume 1 Relevance: {score_1:.4f}\")  # Should be High\n",
    "    print(f\"Resume 2 Relevance: {score_2:.4f}\")  # Should be Low"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
