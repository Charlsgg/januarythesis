{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8f8c25c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Step 1: Loading Raw Data...\n",
      "   - Total Raw Resumes: 34144\n",
      "\n",
      ">>> Step 2: Extracting Experience Sections...\n",
      "\n",
      ">>> Step 3: Applying ROBUST Labels...\n",
      "   - Final Natural Distribution:\n",
      "label\n",
      "Full-time     32615\n",
      "Freelance       415\n",
      "Internship      258\n",
      "Name: count, dtype: int64\n",
      "\n",
      ">>> DONE: Saved 33288 samples to 'robust_experience_training_data_unbalanced.csv'\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import kagglehub\n",
    "from datasets import load_dataset\n",
    "\n",
    "# --- CONFIGURATION ---ssdsa\n",
    "EXPERIENCE_HEADERS = [\n",
    "    r'professional experience', r'work experience', r'employment history',\n",
    "    r'work history', r'experience', r'career history', r'professional background'\n",
    "]\n",
    "\n",
    "NEXT_SECTION_HEADERS = [\n",
    "    r'education', r'academic background', r'skills', r'technical skills',\n",
    "    r'projects', r'personal projects', r'certifications', r'achievements',\n",
    "    r'references', r'languages', r'volunteer', r'interests'\n",
    "]\n",
    "\n",
    "# --- UPDATED CLASSIFICATION REGEX ---\n",
    "# 1. Freelance\n",
    "FREELANCE_REGEX = r'\\b(freelance|freelancer|self-employed|upwork|fiverr|contractor|consultant)\\b'\n",
    "\n",
    "# 2. Intern (Added co-op)\n",
    "INTERN_REGEX    = r'\\b(intern|internship|trainee|summer analyst|student|apprentice|co-op)\\b'\n",
    "\n",
    "# 3. Professional / Senior (UPDATED TO FIX \"DATA SCIENTIST\" ISSUE)\n",
    "# Added: Scientist, Engineer, Developer, Analyst, Associate, etc.\n",
    "PROFESSIONAL_REGEX = r'\\b(senior|manager|lead|principal|head of|chief|executive|years experience|scientist|engineer|developer|analyst|associate|specialist|administrator|officer|architect)\\b'\n",
    "\n",
    "# Contexts where \"Intern\" should be IGNORED\n",
    "IGNORE_INTERN_CONTEXT = r'\\b(managed|mentored|supervised|trained|hired|led|oversaw)\\s+(an\\s+|the\\s+)?'\n",
    "\n",
    "# ==========================================\n",
    "# 1. LOAD DATA\n",
    "# ==========================================\n",
    "def load_data():\n",
    "    print(\">>> Step 1: Loading Raw Data...\")\n",
    "    dfs = []\n",
    "    \n",
    "    # Kaggle\n",
    "    try:\n",
    "        path = kagglehub.dataset_download(\"snehaanbhawal/resume-dataset\")\n",
    "        for root, dirs, files in os.walk(path):\n",
    "            for file in files:\n",
    "                if file.endswith(\".csv\"):\n",
    "                    df = pd.read_csv(os.path.join(root, file))\n",
    "                    if 'Resume_str' in df.columns:\n",
    "                        dfs.append(df[['Resume_str']].rename(columns={'Resume_str': 'text'}))\n",
    "                    break\n",
    "    except: pass\n",
    "\n",
    "    # Hugging Face\n",
    "    try:\n",
    "        dataset = load_dataset(\"InferencePrince555/Resume-Dataset\")\n",
    "        df = dataset['train'].to_pandas()\n",
    "        col = 'Resume_test' if 'Resume_test' in df.columns else df.columns[0]\n",
    "        dfs.append(df[[col]].rename(columns={col: 'text'}))\n",
    "    except: pass\n",
    "\n",
    "    if not dfs: return pd.DataFrame(columns=['text'])\n",
    "    \n",
    "    df_combined = pd.concat(dfs, ignore_index=True).dropna().drop_duplicates(subset=['text'])\n",
    "    print(f\"   - Total Raw Resumes: {len(df_combined)}\")\n",
    "    return df_combined\n",
    "\n",
    "# ==========================================\n",
    "# 2. EXTRACT EXPERIENCE SECTION\n",
    "# ==========================================\n",
    "def extract_experience(text):\n",
    "    text_lower = str(text).lower()\n",
    "    start_idx = -1\n",
    "    for header in EXPERIENCE_HEADERS:\n",
    "        match = re.search(rf'\\b{header}\\b', text_lower)\n",
    "        if match:\n",
    "            if start_idx == -1 or match.start() < start_idx:\n",
    "                start_idx = match.start()\n",
    "    \n",
    "    if start_idx == -1: return None \n",
    "\n",
    "    search_text = text_lower[start_idx:]\n",
    "    end_idx = len(text_lower)\n",
    "    for header in NEXT_SECTION_HEADERS:\n",
    "        match = re.search(rf'\\b{header}\\b', search_text)\n",
    "        if match:\n",
    "            real_match_idx = start_idx + match.start()\n",
    "            if real_match_idx < end_idx:\n",
    "                end_idx = real_match_idx\n",
    "\n",
    "    return text[start_idx:end_idx].strip()\n",
    "\n",
    "# ==========================================\n",
    "# 3. ROBUST LABELING LOGIC (UPDATED)\n",
    "# ==========================================\n",
    "def get_label(text):\n",
    "    if not text: return None\n",
    "    t = text.lower()\n",
    "    \n",
    "    # 1. Count Keywords\n",
    "    n_freelance = len(re.findall(FREELANCE_REGEX, t))\n",
    "    n_professional = len(re.findall(PROFESSIONAL_REGEX, t)) \n",
    "    \n",
    "    # 2. Count Intern keywords INTELLIGENTLY\n",
    "    intern_matches = list(re.finditer(INTERN_REGEX, t))\n",
    "    \n",
    "    valid_intern_count = 0\n",
    "    for match in intern_matches:\n",
    "        start = match.start()\n",
    "        # Look at the 50 characters BEFORE the word \"intern\"\n",
    "        preceding_text = t[max(0, start-50):start]\n",
    "        \n",
    "        # Check if it was preceded by \"managed\", \"mentored\", etc.\n",
    "        if re.search(IGNORE_INTERN_CONTEXT, preceding_text):\n",
    "            continue \n",
    "        valid_intern_count += 1\n",
    "    \n",
    "    # --- CLASSIFICATION RULES ---\n",
    "    \n",
    "    # Rule A: Professional Override (Fixes Data Scientist / Engineer misclassification)\n",
    "    if n_professional > 0:\n",
    "        if n_freelance > n_professional: return 'Freelance'\n",
    "        return 'Full-time'\n",
    "        \n",
    "    # Rule B: Freelance\n",
    "    if n_freelance > 0 and n_freelance >= valid_intern_count: \n",
    "        return 'Freelance'\n",
    "    \n",
    "    # Rule C: Internship\n",
    "    if valid_intern_count > 0: \n",
    "        return 'Internship'\n",
    "    \n",
    "    # Rule D: Default\n",
    "    return 'Full-time'\n",
    "\n",
    "# ==========================================\n",
    "# 4. PROCESSING (NO BALANCING)\n",
    "# ==========================================\n",
    "if __name__ == \"__main__\":\n",
    "    df = load_data()\n",
    "    \n",
    "    print(\"\\n>>> Step 2: Extracting Experience Sections...\")\n",
    "    df['extracted_text'] = df['text'].apply(extract_experience)\n",
    "    df_clean = df.dropna(subset=['extracted_text'])\n",
    "    df_clean = df_clean[df_clean['extracted_text'].str.len() > 50]\n",
    "    \n",
    "    print(\"\\n>>> Step 3: Applying ROBUST Labels...\")\n",
    "    df_clean['label'] = df_clean['extracted_text'].apply(get_label)\n",
    "    \n",
    "    print(\"   - Final Natural Distribution:\")\n",
    "    print(df_clean['label'].value_counts())\n",
    "    \n",
    "    # Preparing final dataframe (Renaming extracted_text to text)\n",
    "    df_training = df_clean[['extracted_text', 'label']].rename(columns={'extracted_text': 'text'})\n",
    "    \n",
    "    # Save\n",
    "    filename = \"robust_experience_training_data_unbalanced.csv\"\n",
    "    df_training.to_csv(filename, index=False)\n",
    "    print(f\"\\n>>> DONE: Saved {len(df_training)} samples to '{filename}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "96734704",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading 'robust_experience_training_data.csv'...\n",
      "\n",
      "================================================================================\n",
      "  CATEGORY: FREELANCE\n",
      "================================================================================\n",
      "\n",
      "Sample #1:\n",
      "--------------------\n",
      "\"experience in IT industry and 3 years of experience in ServiceNow Platform Over 6 years of experience as a QA consultant and was responsible for testing efforts for implementation of all RMS change requests and supported multiple...\"\n",
      "--------------------\n",
      "\n",
      "Sample #2:\n",
      "--------------------\n",
      "\"Experience 01 1996 to Current Consultant Company Name City State Expanded new business opportunities in Texas for Program Management firm Developed contacts with Owners and Architects to develop relationships and solicit project possibilities Provided Project Management and Cost Consulting Services to Owners and Architects on major Higher...\"\n",
      "--------------------\n",
      "\n",
      "Sample #3:\n",
      "--------------------\n",
      "\"experience in IT industry and 3 years of experience in ServiceNow Platform Over 6 years of experience as a QA consultant and was responsible for testing efforts for implementation of all RMS change requests and supported multiple...\"\n",
      "--------------------\n",
      "\n",
      "================================================================================\n",
      "  CATEGORY: INTERNSHIP\n",
      "================================================================================\n",
      "\n",
      "Sample #1:\n",
      "--------------------\n",
      "\"Experience Fitness Instructor 02 2013 to Current Company Name City State Teach energetic workouts that are challenging and motivating yet safe Provide students individualized hands on adjustment throughout class Address each student by name during class and cultivate long term relationships Organized the 2014 Fitness Kick off Challenge in which local vendors provided educational seminars products ...\"\n",
      "--------------------\n",
      "\n",
      "Sample #2:\n",
      "--------------------\n",
      "\"experience and 6 months of Engineering Internship experience working on multi million dollar...\"\n",
      "--------------------\n",
      "\n",
      "Sample #3:\n",
      "--------------------\n",
      "\"Work Experience Intern Minnesota Pollution Control Agency Rochester MN June 2018 to March 2019 Create Maps in ArcGIS Monitor stream sites on the North Branch of the Whitewater River Collaborate with team members on various field work tasks including installation of longterm nitrate monitoring sites deployment of Sondes and HOBO temperature loggers and invertebrate sampling Graduate Assistant WSU R...\"\n",
      "--------------------\n",
      "\n",
      "================================================================================\n",
      "  CATEGORY: FULL-TIME\n",
      "================================================================================\n",
      "\n",
      "Sample #1:\n",
      "--------------------\n",
      "\"experience in the IT industry in support of national security objectives leveraging trained teams and experience in the innovation of protecting against cybersecurity threats and ensuring continuity of business operations Formal and specialized training in major areas of IT network cybersecurity and monitoring Excellent organization and communication...\"\n",
      "--------------------\n",
      "\n",
      "Sample #2:\n",
      "--------------------\n",
      "\"Work Experience IT SYSTEMS ENGINEER VENMILL INDUSTRIES INC February 2018 to Present Technical lead of IT Infrastructure Virtualization Networking Storage System administrator Database and computer security auditing Serve as IT technical advisor for Administration Operation day to day operations Prepare ITIS financial budgets and present project proposals to VP and executives of VenMill Inc Pointus...\"\n",
      "--------------------\n",
      "\n",
      "Sample #3:\n",
      "--------------------\n",
      "\"experience UI Front end development Authorized to work in the US for any employer Work Experience Front End Developer CARE USA Atlanta GA January 2016 to August 2017 Contract Developed various CARE web sites CARE Journeys Young Professionals SXD Scale Design Impact Magazine CARE Board Update CARE main web site Drupal content maintain web assets Worked on various web campaigns and initiatives worke...\"\n",
      "--------------------\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Check if the file from the previous step exists\n",
    "filename = \"robust_experience_training_data.csv\"\n",
    "\n",
    "if os.path.exists(filename):\n",
    "    print(f\"Loading '{filename}'...\")\n",
    "    df = pd.read_csv(filename)\n",
    "    \n",
    "    # --- VISUALIZATION FUNCTION ---\n",
    "    def view_samples(category):\n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(f\"  CATEGORY: {category.upper()}\")\n",
    "        print(\"=\"*80)\n",
    "        \n",
    "        # Get random samples for this category\n",
    "        subset = df[df['label'] == category]\n",
    "        \n",
    "        if subset.empty:\n",
    "            print(\"  (No samples found)\")\n",
    "            return\n",
    "\n",
    "        samples = subset.sample(n=min(3, len(subset)), random_state=42)\n",
    "        \n",
    "        for i, row in enumerate(samples.itertuples()):\n",
    "            print(f\"\\nSample #{i+1}:\")\n",
    "            print(\"-\" * 20)\n",
    "            \n",
    "            # Show the first 400 characters of the extracted text\n",
    "            text_preview = str(row.text)[:400].replace('\\n', ' ')\n",
    "            print(f\"\\\"{text_preview}...\\\"\")\n",
    "            print(\"-\" * 20)\n",
    "\n",
    "    # --- RUN FOR EACH CLASS ---\n",
    "    view_samples('Freelance')\n",
    "    view_samples('Internship')\n",
    "    view_samples('Full-time')\n",
    "\n",
    "else:\n",
    "    print(f\"Error: '{filename}' not found.\")\n",
    "    print(\"Please run the 'Final Data Preparation' script (previous step) first to generate the file.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17cdfbcb",
   "metadata": {},
   "source": [
    "suppose for data balancing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "9d5d75f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading BERT model for augmentation (this happens once)...\n",
      "Loading data...\n",
      "Downsampling Full-time data...\n",
      "\n",
      "--- Augmenting Freelance ---\n",
      "Original: 415 | Needed: 2919\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|â–‰         | 283/2919 [09:10<1:25:31,  1.95s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[42], line 55\u001b[0m\n\u001b[0;32m     51\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(new_samples) \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m needed: \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m     53\u001b[0m \u001b[38;5;66;03m# Generate Augmented Text\u001b[39;00m\n\u001b[0;32m     54\u001b[0m \u001b[38;5;66;03m# aug.augment returns a list, we take the first item\u001b[39;00m\n\u001b[1;32m---> 55\u001b[0m augmented_text \u001b[38;5;241m=\u001b[39m \u001b[43maug\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maugment\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     57\u001b[0m \u001b[38;5;66;03m# Sometimes it returns a list, handle that:\u001b[39;00m\n\u001b[0;32m     58\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(augmented_text, \u001b[38;5;28mlist\u001b[39m):\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\nlpaug\\base_augmenter.py:98\u001b[0m, in \u001b[0;36mAugmenter.augment\u001b[1;34m(self, data, n, num_thread)\u001b[0m\n\u001b[0;32m     96\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAbstSummAug\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mBackTranslationAug\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mContextualWordEmbsAug\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mContextualWordEmbsForSentenceAug\u001b[39m\u001b[38;5;124m'\u001b[39m]:\n\u001b[0;32m     97\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(aug_num):\n\u001b[1;32m---> 98\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[43maction_fx\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclean_data\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     99\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(result, \u001b[38;5;28mlist\u001b[39m):\n\u001b[0;32m    100\u001b[0m             augmented_results\u001b[38;5;241m.\u001b[39mextend(result)\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\nlpaug\\augmenter\\word\\context_word_embs.py:471\u001b[0m, in \u001b[0;36mContextualWordEmbsAug.substitute\u001b[1;34m(self, data)\u001b[0m\n\u001b[0;32m    468\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(masked_texts):\n\u001b[0;32m    469\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[1;32m--> 471\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmasked_texts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_words\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moriginal_tokens\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    473\u001b[0m \u001b[38;5;66;03m# Update doc\u001b[39;00m\n\u001b[0;32m    474\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m original_token, aug_input_pos, output, masked_text \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(original_tokens, aug_input_poses, outputs, masked_texts):\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\nlpaug\\model\\lang_models\\bert.py:105\u001b[0m, in \u001b[0;36mBert.predict\u001b[1;34m(self, texts, target_words, n)\u001b[0m\n\u001b[0;32m    103\u001b[0m \u001b[38;5;66;03m# Prediction\u001b[39;00m\n\u001b[0;32m    104\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m--> 105\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmask_inputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    107\u001b[0m \u001b[38;5;66;03m# Selection\u001b[39;00m\n\u001b[0;32m    108\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m output, target_pos, target_token \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(outputs[\u001b[38;5;241m0\u001b[39m], target_poses, target_words):\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\models\\bert\\modeling_bert.py:1461\u001b[0m, in \u001b[0;36mBertForMaskedLM.forward\u001b[1;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, labels, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m   1452\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1453\u001b[0m \u001b[38;5;124;03mlabels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\u001b[39;00m\n\u001b[0;32m   1454\u001b[0m \u001b[38;5;124;03m    Labels for computing the masked language modeling loss. Indices should be in `[-100, 0, ...,\u001b[39;00m\n\u001b[0;32m   1455\u001b[0m \u001b[38;5;124;03m    config.vocab_size]` (see `input_ids` docstring) Tokens with indices set to `-100` are ignored (masked), the\u001b[39;00m\n\u001b[0;32m   1456\u001b[0m \u001b[38;5;124;03m    loss is only computed for the tokens with labels in `[0, ..., config.vocab_size]`\u001b[39;00m\n\u001b[0;32m   1457\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1459\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[1;32m-> 1461\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbert\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1462\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1463\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1464\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken_type_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1465\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1466\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1467\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1468\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1469\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1470\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1471\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1472\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1473\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1475\u001b[0m sequence_output \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m   1476\u001b[0m prediction_scores \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcls(sequence_output)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\models\\bert\\modeling_bert.py:1142\u001b[0m, in \u001b[0;36mBertModel.forward\u001b[1;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m   1135\u001b[0m \u001b[38;5;66;03m# Prepare head mask if needed\u001b[39;00m\n\u001b[0;32m   1136\u001b[0m \u001b[38;5;66;03m# 1.0 in head_mask indicate we keep the head\u001b[39;00m\n\u001b[0;32m   1137\u001b[0m \u001b[38;5;66;03m# attention_probs has shape bsz x n_heads x N x N\u001b[39;00m\n\u001b[0;32m   1138\u001b[0m \u001b[38;5;66;03m# input head_mask has shape [num_heads] or [num_hidden_layers x num_heads]\u001b[39;00m\n\u001b[0;32m   1139\u001b[0m \u001b[38;5;66;03m# and head_mask is converted to shape [num_hidden_layers x batch x num_heads x seq_length x seq_length]\u001b[39;00m\n\u001b[0;32m   1140\u001b[0m head_mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_head_mask(head_mask, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mnum_hidden_layers)\n\u001b[1;32m-> 1142\u001b[0m encoder_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1143\u001b[0m \u001b[43m    \u001b[49m\u001b[43membedding_output\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1144\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1145\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1146\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1147\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_extended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1148\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1149\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1150\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1151\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1152\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1153\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1154\u001b[0m sequence_output \u001b[38;5;241m=\u001b[39m encoder_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m   1155\u001b[0m pooled_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooler(sequence_output) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\models\\bert\\modeling_bert.py:695\u001b[0m, in \u001b[0;36mBertEncoder.forward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m    684\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[0;32m    685\u001b[0m         layer_module\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m,\n\u001b[0;32m    686\u001b[0m         hidden_states,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    692\u001b[0m         output_attentions,\n\u001b[0;32m    693\u001b[0m     )\n\u001b[0;32m    694\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 695\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mlayer_module\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    696\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    697\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    698\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    699\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    700\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    701\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    702\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    703\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    705\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    706\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache:\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\models\\bert\\modeling_bert.py:627\u001b[0m, in \u001b[0;36mBertLayer.forward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[0;32m    624\u001b[0m     cross_attn_present_key_value \u001b[38;5;241m=\u001b[39m cross_attention_outputs[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m    625\u001b[0m     present_key_value \u001b[38;5;241m=\u001b[39m present_key_value \u001b[38;5;241m+\u001b[39m cross_attn_present_key_value\n\u001b[1;32m--> 627\u001b[0m layer_output \u001b[38;5;241m=\u001b[39m \u001b[43mapply_chunking_to_forward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    628\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfeed_forward_chunk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchunk_size_feed_forward\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mseq_len_dim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_output\u001b[49m\n\u001b[0;32m    629\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    630\u001b[0m outputs \u001b[38;5;241m=\u001b[39m (layer_output,) \u001b[38;5;241m+\u001b[39m outputs\n\u001b[0;32m    632\u001b[0m \u001b[38;5;66;03m# if decoder, return the attn key/values as the last output\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\pytorch_utils.py:258\u001b[0m, in \u001b[0;36mapply_chunking_to_forward\u001b[1;34m(forward_fn, chunk_size, chunk_dim, *input_tensors)\u001b[0m\n\u001b[0;32m    255\u001b[0m     \u001b[38;5;66;03m# concatenate output at same dimension\u001b[39;00m\n\u001b[0;32m    256\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcat(output_chunks, dim\u001b[38;5;241m=\u001b[39mchunk_dim)\n\u001b[1;32m--> 258\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minput_tensors\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\models\\bert\\modeling_bert.py:640\u001b[0m, in \u001b[0;36mBertLayer.feed_forward_chunk\u001b[1;34m(self, attention_output)\u001b[0m\n\u001b[0;32m    638\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfeed_forward_chunk\u001b[39m(\u001b[38;5;28mself\u001b[39m, attention_output):\n\u001b[0;32m    639\u001b[0m     intermediate_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mintermediate(attention_output)\n\u001b[1;32m--> 640\u001b[0m     layer_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutput\u001b[49m\u001b[43m(\u001b[49m\u001b[43mintermediate_output\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_output\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    641\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m layer_output\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\models\\bert\\modeling_bert.py:554\u001b[0m, in \u001b[0;36mBertOutput.forward\u001b[1;34m(self, hidden_states, input_tensor)\u001b[0m\n\u001b[0;32m    552\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdense(hidden_states)\n\u001b[0;32m    553\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(hidden_states)\n\u001b[1;32m--> 554\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mLayerNorm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43minput_tensor\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    555\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m hidden_states\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\nn\\modules\\normalization.py:217\u001b[0m, in \u001b[0;36mLayerNorm.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    216\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 217\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayer_norm\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    218\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnormalized_shape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meps\u001b[49m\n\u001b[0;32m    219\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\nn\\functional.py:2900\u001b[0m, in \u001b[0;36mlayer_norm\u001b[1;34m(input, normalized_shape, weight, bias, eps)\u001b[0m\n\u001b[0;32m   2890\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_variadic(\u001b[38;5;28minput\u001b[39m, weight, bias):\n\u001b[0;32m   2891\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m   2892\u001b[0m         layer_norm,\n\u001b[0;32m   2893\u001b[0m         (\u001b[38;5;28minput\u001b[39m, weight, bias),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2898\u001b[0m         eps\u001b[38;5;241m=\u001b[39meps,\n\u001b[0;32m   2899\u001b[0m     )\n\u001b[1;32m-> 2900\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayer_norm\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2901\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnormalized_shape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackends\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcudnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menabled\u001b[49m\n\u001b[0;32m   2902\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.utils import resample\n",
    "import nlpaug.augmenter.word as naw\n",
    "import os\n",
    "from tqdm import tqdm  # For a progress bar\n",
    "\n",
    "# --- CONFIGURATION ---\n",
    "INPUT_FILE = \"robust_experience_training_data_unbalanced.csv\"\n",
    "OUTPUT_FILE = \"robust_experience_training_data_bert_augmented.csv\"\n",
    "TARGET_PER_CLASS = 3334\n",
    "\n",
    "# --- LOAD BERT AUGMENTER ---\n",
    "# This downloads a small BERT model specifically for inserting contextual words\n",
    "print(\"Loading BERT model for augmentation (this happens once)...\")\n",
    "aug = naw.ContextualWordEmbsAug(\n",
    "    model_path='bert-base-uncased', \n",
    "    action=\"substitute\",\n",
    "    device='cpu' # Change to 'cuda' if you have a GPU\n",
    ")\n",
    "\n",
    "# --- MAIN PROCESS ---\n",
    "if not os.path.exists(INPUT_FILE):\n",
    "    print(f\"Error: {INPUT_FILE} not found.\")\n",
    "else:\n",
    "    print(\"Loading data...\")\n",
    "    df = pd.read_csv(INPUT_FILE)\n",
    "    \n",
    "    # 1. Downsample Majority (Full-time) - No augmentation needed here\n",
    "    print(\"Downsampling Full-time data...\")\n",
    "    df_fulltime = df[df['label'] == 'Full-time']\n",
    "    df_fulltime_bal = resample(df_fulltime, replace=False, n_samples=TARGET_PER_CLASS, random_state=42)\n",
    "\n",
    "    # 2. Augment Minorities (Freelance & Internship)\n",
    "    augmented_dfs = [df_fulltime_bal]\n",
    "    \n",
    "    for category in ['Freelance', 'Internship']:\n",
    "        subset = df[df['label'] == category]\n",
    "        current_count = len(subset)\n",
    "        needed = TARGET_PER_CLASS - current_count\n",
    "        \n",
    "        print(f\"\\n--- Augmenting {category} ---\")\n",
    "        print(f\"Original: {current_count} | Needed: {needed}\")\n",
    "        \n",
    "        new_samples = []\n",
    "        \n",
    "        # We use a progress bar because BERT is slower than NLTK\n",
    "        with tqdm(total=needed) as pbar:\n",
    "            while len(new_samples) < needed:\n",
    "                # Iterate through original samples\n",
    "                for text in subset['text']:\n",
    "                    if len(new_samples) >= needed: break\n",
    "                    \n",
    "                    # Generate Augmented Text\n",
    "                    # aug.augment returns a list, we take the first item\n",
    "                    augmented_text = aug.augment(str(text))\n",
    "                    \n",
    "                    # Sometimes it returns a list, handle that:\n",
    "                    if isinstance(augmented_text, list):\n",
    "                        augmented_text = augmented_text[0]\n",
    "                    \n",
    "                    # Add to list if it's not identical (BERT rarely produces identicals)\n",
    "                    if augmented_text != text:\n",
    "                        new_samples.append({'label': category, 'text': augmented_text})\n",
    "                        pbar.update(1)\n",
    "\n",
    "        df_synthetic = pd.DataFrame(new_samples)\n",
    "        df_combined = pd.concat([subset, df_synthetic])\n",
    "        augmented_dfs.append(df_combined)\n",
    "\n",
    "    # 3. Final Combine & Save\n",
    "    df_final = pd.concat(augmented_dfs)\n",
    "    df_final = df_final.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "    print(\"\\n\" + \"=\"*30)\n",
    "    print(\"FINAL DATASET STATS\")\n",
    "    print(\"=\"*30)\n",
    "    print(df_final['label'].value_counts())\n",
    "    \n",
    "    df_final.to_csv(OUTPUT_FILE, index=False)\n",
    "    print(f\"\\nSaved HIGH-QUALITY dataset to '{OUTPUT_FILE}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97532e73",
   "metadata": {},
   "source": [
    "full training mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "d9566b92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Loading Data & Splitting...\n",
      "Classes: ['Freelance' 'Full-time' 'Internship']\n",
      "Training Samples (Raw): 26630\n",
      "Test Samples (Pure): 6658\n",
      "\n",
      "2. Augmenting Training Data (BERT)...\n",
      "   Augmenting Freelance: Creating 2668 new samples...\n",
      "   Augmenting Internship: Creating 2794 new samples...\n",
      "Final Training Set Size: 9000\n",
      "label\n",
      "Internship    3000\n",
      "Freelance     3000\n",
      "Full-time     3000\n",
      "Name: count, dtype: int64\n",
      "\n",
      "3. Training BERT...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\charls\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 563/563 [04:06<00:00,  2.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Avg Loss: 0.35194173762591335\n",
      "Epoch 2/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 563/563 [03:58<00:00,  2.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Avg Loss: 0.10765275600172751\n",
      "Epoch 3/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 563/563 [04:01<00:00,  2.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Avg Loss: 0.0458391964432269\n",
      "Epoch 4/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 563/563 [04:00<00:00,  2.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Avg Loss: 0.025387587201315082\n",
      "\n",
      "4. Final Evaluation (On Pure Unseen Data)...\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   Freelance       0.30      0.78      0.43        83\n",
      "   Full-time       1.00      0.96      0.98      6523\n",
      "  Internship       0.29      0.88      0.44        52\n",
      "\n",
      "    accuracy                           0.96      6658\n",
      "   macro avg       0.53      0.88      0.62      6658\n",
      "weighted avg       0.98      0.96      0.97      6658\n",
      "\n",
      "Model saved.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import nlpaug.augmenter.word as naw\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, AdamW\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.utils import resample\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "# --- CONFIGURATION ---\n",
    "INPUT_FILE = \"robust_experience_training_data_unbalanced.csv\" # USE THE ORIGINAL UNBALANCED FILE\n",
    "SAVE_PATH = \"./saved_bert_model_v2\"\n",
    "MAX_LEN = 128\n",
    "BATCH_SIZE = 16\n",
    "EPOCHS = 4\n",
    "TARGET_PER_CLASS = 3000 # Augment up to this number\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# --- 1. LOAD & SPLIT FIRST (CRITICAL STEP) ---\n",
    "if not os.path.exists(INPUT_FILE):\n",
    "    raise FileNotFoundError(\"Please provide the original 'unbalanced' csv file.\")\n",
    "\n",
    "print(\"1. Loading Data & Splitting...\")\n",
    "df = pd.read_csv(INPUT_FILE)\n",
    "\n",
    "# Encode Labels\n",
    "le = LabelEncoder()\n",
    "df['label_encoded'] = le.fit_transform(df['label'])\n",
    "classes = le.classes_\n",
    "print(f\"Classes: {classes}\")\n",
    "\n",
    "# SPLIT FIRST! Keep 20% pure for testing.\n",
    "X_train_raw, X_test, y_train_raw, y_test = train_test_split(\n",
    "    df['text'].values, \n",
    "    df['label_encoded'].values, \n",
    "    test_size=0.2, \n",
    "    random_state=42, \n",
    "    stratify=df['label_encoded']\n",
    ")\n",
    "\n",
    "# Reassemble Training Set for Augmentation\n",
    "train_df = pd.DataFrame({'text': X_train_raw, 'label_encoded': y_train_raw})\n",
    "train_df['label'] = le.inverse_transform(train_df['label_encoded'])\n",
    "\n",
    "print(f\"Training Samples (Raw): {len(train_df)}\")\n",
    "print(f\"Test Samples (Pure): {len(X_test)}\")\n",
    "\n",
    "# --- 2. AUGMENT ONLY TRAINING DATA ---\n",
    "print(\"\\n2. Augmenting Training Data (BERT)...\")\n",
    "aug = naw.ContextualWordEmbsAug(model_path='bert-base-uncased', action=\"substitute\", device='cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "augmented_dfs = []\n",
    "\n",
    "for class_name in classes:\n",
    "    # Get all samples for this class\n",
    "    subset = train_df[train_df['label'] == class_name]\n",
    "    \n",
    "    # If Majority Class (Full-time), Downsample\n",
    "    if len(subset) > TARGET_PER_CLASS:\n",
    "        subset = resample(subset, replace=False, n_samples=TARGET_PER_CLASS, random_state=42)\n",
    "        augmented_dfs.append(subset)\n",
    "    \n",
    "    # If Minority Class, Augment\n",
    "    else:\n",
    "        # Add original samples first\n",
    "        augmented_dfs.append(subset)\n",
    "        \n",
    "        needed = TARGET_PER_CLASS - len(subset)\n",
    "        print(f\"   Augmenting {class_name}: Creating {needed} new samples...\")\n",
    "        \n",
    "        new_texts = []\n",
    "        original_texts = subset['text'].tolist()\n",
    "        \n",
    "        # Cycle through originals to create new ones\n",
    "        while len(new_texts) < needed:\n",
    "            for text in original_texts:\n",
    "                if len(new_texts) >= needed: break\n",
    "                try:\n",
    "                    # Augment\n",
    "                    aug_text = aug.augment(str(text))\n",
    "                    if isinstance(aug_text, list): aug_text = aug_text[0]\n",
    "                    if aug_text != text:\n",
    "                        new_texts.append(aug_text)\n",
    "                except:\n",
    "                    continue\n",
    "        \n",
    "        # Add new synthetic samples\n",
    "        temp_df = pd.DataFrame({'text': new_texts})\n",
    "        temp_df['label'] = class_name\n",
    "        temp_df['label_encoded'] = le.transform([class_name])[0]\n",
    "        augmented_dfs.append(temp_df)\n",
    "\n",
    "# Combine\n",
    "df_train_final = pd.concat(augmented_dfs).sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "print(f\"Final Training Set Size: {len(df_train_final)}\")\n",
    "print(df_train_final['label'].value_counts())\n",
    "\n",
    "# --- 3. PREPARE DATASETS ---\n",
    "class ResumeDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_len):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        text = str(self.texts[item])\n",
    "        label = self.labels[item]\n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            text, add_special_tokens=True, max_length=self.max_len,\n",
    "            return_token_type_ids=False, padding='max_length',\n",
    "            truncation=True, return_attention_mask=True, return_tensors='pt',\n",
    "        )\n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'labels': torch.tensor(label, dtype=torch.long)\n",
    "        }\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "train_dataset = ResumeDataset(df_train_final['text'].values, df_train_final['label_encoded'].values, tokenizer, MAX_LEN)\n",
    "test_dataset = ResumeDataset(X_test, y_test, tokenizer, MAX_LEN)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE)\n",
    "\n",
    "# --- 4. TRAIN ---\n",
    "print(\"\\n3. Training BERT...\")\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=len(classes))\n",
    "model = model.to(DEVICE)\n",
    "optimizer = AdamW(model.parameters(), lr=2e-5)\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    print(f\"Epoch {epoch+1}/{EPOCHS}\")\n",
    "    \n",
    "    for batch in tqdm(train_loader):\n",
    "        input_ids = batch['input_ids'].to(DEVICE)\n",
    "        mask = batch['attention_mask'].to(DEVICE)\n",
    "        labels = batch['labels'].to(DEVICE)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(input_ids, attention_mask=mask, labels=labels)\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "        \n",
    "    print(f\"   Avg Loss: {total_loss / len(train_loader)}\")\n",
    "\n",
    "# --- 5. EVALUATE ---\n",
    "print(\"\\n4. Final Evaluation (On Pure Unseen Data)...\")\n",
    "model.eval()\n",
    "preds, true_labels = [], []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in test_loader:\n",
    "        input_ids = batch['input_ids'].to(DEVICE)\n",
    "        mask = batch['attention_mask'].to(DEVICE)\n",
    "        outputs = model(input_ids, attention_mask=mask)\n",
    "        _, prediction = torch.max(outputs.logits, dim=1)\n",
    "        preds.extend(prediction.cpu().numpy())\n",
    "        true_labels.extend(batch['labels'].numpy())\n",
    "\n",
    "print(classification_report(true_labels, preds, target_names=classes))\n",
    "\n",
    "# Save\n",
    "if not os.path.exists(SAVE_PATH): os.makedirs(SAVE_PATH)\n",
    "model.save_pretrained(SAVE_PATH)\n",
    "tokenizer.save_pretrained(SAVE_PATH)\n",
    "np.save(os.path.join(SAVE_PATH, 'classes.npy'), classes)\n",
    "print(\"Model saved.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ac501db",
   "metadata": {},
   "source": [
    "this part it will prepare for testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cd26c0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading BERT model from ./saved_bert_model...\n",
      "âœ… Model Loaded Successfully!\n",
      "Using Device: cuda\n",
      "Classes: ['Freelance' 'Full-time' 'Internship']\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "import os\n",
    "\n",
    "# --- CONFIGURATION ---\n",
    "MODEL_PATH = \"./saved_bert_model_v2\"\n",
    "MAX_LEN = 128\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# --- LOAD MODEL & TOKENIZER ---\n",
    "if not os.path.exists(MODEL_PATH):\n",
    "    print(f\"Error: Model not found at {MODEL_PATH}. Did you run the training cell?\")\n",
    "else:\n",
    "    print(f\"Loading BERT model from {MODEL_PATH}...\")\n",
    "    \n",
    "    # Load architecture and weights\n",
    "    model = BertForSequenceClassification.from_pretrained(MODEL_PATH)\n",
    "    model = model.to(device)\n",
    "    model.eval() # Freeze for inference\n",
    "\n",
    "    # Load tokenizer\n",
    "    tokenizer = BertTokenizer.from_pretrained(MODEL_PATH)\n",
    "\n",
    "    # Load class names\n",
    "    classes = np.load(os.path.join(MODEL_PATH, 'classes.npy'), allow_pickle=True)\n",
    "    \n",
    "    print(f\"âœ… Model Loaded Successfully!\")\n",
    "    print(f\"Using Device: {device}\")\n",
    "    print(f\"Classes: {classes}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b3fe0d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_resume(text):\n",
    "    \"\"\"\n",
    "    Accepts a resume string and returns the predicted label and confidence score.\n",
    "    \"\"\"\n",
    "    if not text: return \"Empty\", 0.0\n",
    "\n",
    "    # 1. Tokenize\n",
    "    encoding = tokenizer.encode_plus(\n",
    "        text,\n",
    "        add_special_tokens=True,\n",
    "        max_length=MAX_LEN,\n",
    "        return_token_type_ids=False,\n",
    "        padding='max_length',\n",
    "        truncation=True,\n",
    "        return_attention_mask=True,\n",
    "        return_tensors='pt',\n",
    "    )\n",
    "\n",
    "    # 2. Move to GPU/CPU\n",
    "    input_ids = encoding['input_ids'].to(device)\n",
    "    attention_mask = encoding['attention_mask'].to(device)\n",
    "\n",
    "    # 3. Predict\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        logits = outputs.logits\n",
    "        probs = torch.nn.functional.softmax(logits, dim=1)\n",
    "    \n",
    "    # 4. Decode Result\n",
    "    confidence, prediction_idx = torch.max(probs, dim=1)\n",
    "    predicted_label = classes[prediction_idx.item()]\n",
    "    confidence_score = confidence.item()\n",
    "\n",
    "    return predicted_label, confidence_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "39a50cc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PREDICTION      | CONF.    | RESUME TEXT\n",
      "==========================================================================================\n",
      "FULL-TIME       | 100.0% âœ… | \"I spent my summer assisting the backend team with API docume...\"\n",
      "FULL-TIME       | 80.5% âš ï¸ | \"Shadowed the Senior UX Designer and helped conduct user rese...\"\n",
      "INTERNSHIP      | 100.0% âœ… | \"Part of the university co-op program, working on data entry ...\"\n",
      "FULL-TIME       | 100.0% âœ… | \"I take on various graphic design projects on Upwork, managin...\"\n",
      "FULL-TIME       | 100.0% âœ… | \"Served as the Lead Developer for 4 years, managing a team of...\"\n",
      "FULL-TIME       | 100.0% âœ… | \"Responsible for the end-to-end deployment of the company's m...\"\n",
      "FULL-TIME       | 99.9% âœ… | \"Employed since 2018 as a Senior Analyst, handling daily oper...\"\n",
      "FULL-TIME       | 90.2% âœ… | \"I worked for 6 months covering a maternity leave, handling f...\"\n",
      "FULL-TIME       | 93.5% âœ… | \"I built the entire mobile app by myself during the weekends ...\"\n",
      "FULL-TIME       | 100.0% âœ… | \"Junior developer responsible for updating the UI, reporting ...\"\n"
     ]
    }
   ],
   "source": [
    "# --- LIST OF TEST CASES ---\n",
    "test_cases = [\n",
    "    # --- INTERNSHIP EXAMPLES ---\n",
    "    \"I spent my summer assisting the backend team with API documentation and minor bug fixes.\",\n",
    "    \"Shadowed the Senior UX Designer and helped conduct user research surveys for 3 months.\",\n",
    "    \"Part of the university co-op program, working on data entry and basic SQL queries.\",\n",
    "    \n",
    "    # --- FREELANCE EXAMPLES ---\n",
    "    \"I take on various graphic design projects on Upwork, managing my own schedule and clients.\",\n",
    "\n",
    "    \n",
    "    # --- FULL-TIME EXAMPLES ---\n",
    "    \"Served as the Lead Developer for 4 years, managing a team of 10 engineers.\",\n",
    "    \"Responsible for the end-to-end deployment of the company's main payment gateway.\",\n",
    "    \"Employed since 2018 as a Senior Analyst, handling daily operations and quarterly reporting.\",\n",
    "    \n",
    "    # --- TRICKY / AMBIGUOUS EXAMPLES (The real test!) ---\n",
    "    \"I worked for 6 months covering a maternity leave, handling full server access.\", \n",
    "    # ^ (Could be Full-time or Contract/Freelance)\n",
    "    \n",
    "    \"I built the entire mobile app by myself during the weekends while studying.\", \n",
    "    # ^ (Likely Freelance or Personal Project, shouldn't be Full-time)\n",
    "    \n",
    "    \"Junior developer responsible for updating the UI, reporting to the CTO.\" \n",
    "    # ^ (Context implies Full-time, but description is simple)\n",
    "]\n",
    "\n",
    "# --- RUN BATCH PREDICTION ---\n",
    "print(f\"{'PREDICTION':<15} | {'CONF.':<8} | {'RESUME TEXT'}\")\n",
    "print(\"=\"*90)\n",
    "\n",
    "for text in test_cases:\n",
    "    label, score = predict_resume(text)\n",
    "    \n",
    "    # Color coding for easier reading (optional)\n",
    "    # 90%+ confidence = High, <70% = Low\n",
    "    indicator = \"âœ…\" if score > 0.9 else \"âš ï¸\"\n",
    "    \n",
    "    print(f\"{label.upper():<15} | {score*100:.1f}% {indicator} | \\\"{text[:60]}...\\\"\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
