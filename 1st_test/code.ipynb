{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f8f8c25c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Step 0: Loading AI Model...\n",
      "WARNING:tensorflow:From C:\\Users\\charls\\AppData\\Roaming\\Python\\Python311\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Step 1: Loading Raw Data...\n",
      "Warning: Looks like you're using an outdated `kagglehub` version (installed: 0.3.13), please consider upgrading to the latest version (0.4.0).\n",
      "   - Total Raw Resumes: 2482\n",
      "\n",
      ">>> Step 2: Extracting Experience Sections...\n",
      "\n",
      ">>> Step 3: AI Labeling (2380 samples) using Optimized Dataset Pipeline...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2380/2380 [30:50<00:00,  1.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "   - Final AI Distribution:\n",
      "label\n",
      "Full-time     2114\n",
      "Freelance      152\n",
      "Internship     114\n",
      "Name: count, dtype: int64\n",
      "\n",
      ">>> DONE: Saved to 'ai_labeled_experience_data.csv'\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "import kagglehub\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from transformers import pipeline\n",
    "from tqdm import tqdm\n",
    "from transformers.pipelines.pt_utils import KeyDataset\n",
    "from datasets import Dataset\n",
    "# --- CONFIGURATION ---\n",
    "EXPERIENCE_HEADERS = [\n",
    "    r'professional experience', r'work experience', r'employment history',\n",
    "    r'work history', r'experience', r'career history', r'professional background'\n",
    "]\n",
    "\n",
    "NEXT_SECTION_HEADERS = [\n",
    "    r'education', r'academic background', r'skills', r'technical skills',\n",
    "    r'projects', r'personal projects', r'certifications', r'achievements',\n",
    "    r'references', r'languages', r'volunteer', r'interests'\n",
    "]\n",
    "\n",
    "# ==========================================\n",
    "# 1. INITIALIZE ZERO-SHOT AI\n",
    "# ==========================================\n",
    "print(\">>> Step 0: Loading AI Model...\")\n",
    "device = 0 if torch.cuda.is_available() else -1\n",
    "classifier = pipeline(\n",
    "    \"zero-shot-classification\", \n",
    "    model=\"facebook/bart-large-mnli\", \n",
    "    device=device\n",
    ")\n",
    "\n",
    "# Descriptive labels help the AI understand context better\n",
    "CANDIDATE_LABELS = [\n",
    "    \"Full-time employee with professional experience\", \n",
    "    \"Freelance contractor or self-employed consultant\", \n",
    "    \"Student intern or trainee\"\n",
    "]\n",
    "\n",
    "# Map back to your simple categories\n",
    "LABEL_MAP = {\n",
    "    \"Full-time employee with professional experience\": \"Full-time\",\n",
    "    \"Freelance contractor or self-employed consultant\": \"Freelance\",\n",
    "    \"Student intern or trainee\": \"Internship\"\n",
    "}\n",
    "\n",
    "# ==========================================\n",
    "# 2. LOAD DATA\n",
    "# ==========================================\n",
    "def load_data():\n",
    "    print(\">>> Step 1: Loading Raw Data...\")\n",
    "    dfs = []\n",
    "    \n",
    "    # Kaggle\n",
    "    try:\n",
    "        path = kagglehub.dataset_download(\"snehaanbhawal/resume-dataset\")\n",
    "        for root, dirs, files in os.walk(path):\n",
    "            for file in files:\n",
    "                if file.endswith(\".csv\"):\n",
    "                    df = pd.read_csv(os.path.join(root, file))\n",
    "                    if 'Resume_str' in df.columns:\n",
    "                        dfs.append(df[['Resume_str']].rename(columns={'Resume_str': 'text'}))\n",
    "                    break\n",
    "    except: pass\n",
    "\n",
    "    # # Hugging Face\n",
    "    # try:\n",
    "    #     dataset = load_dataset(\"InferencePrince555/Resume-Dataset\")\n",
    "    #     df = dataset['train'].to_pandas()\n",
    "    #     col = 'Resume_test' if 'Resume_test' in df.columns else df.columns[0]\n",
    "    #     dfs.append(df[[col]].rename(columns={col: 'text'}))\n",
    "    # except: pass\n",
    "\n",
    "    # if not dfs: return pd.DataFrame(columns=['text'])\n",
    "    \n",
    "    df_combined = pd.concat(dfs, ignore_index=True).dropna().drop_duplicates(subset=['text'])\n",
    "    print(f\"   - Total Raw Resumes: {len(df_combined)}\")\n",
    "    return df_combined\n",
    "\n",
    "# ==========================================\n",
    "# 3. EXTRACT EXPERIENCE SECTION\n",
    "# ==========================================\n",
    "def extract_experience(text):\n",
    "    text_lower = str(text).lower()\n",
    "    start_idx = -1\n",
    "    for header in EXPERIENCE_HEADERS:\n",
    "        match = re.search(rf'\\b{header}\\b', text_lower)\n",
    "        if match:\n",
    "            if start_idx == -1 or match.start() < start_idx:\n",
    "                start_idx = match.start()\n",
    "    \n",
    "    if start_idx == -1: return None \n",
    "\n",
    "    search_text = text_lower[start_idx:]\n",
    "    end_idx = len(text_lower)\n",
    "    for header in NEXT_SECTION_HEADERS:\n",
    "        match = re.search(rf'\\b{header}\\b', search_text)\n",
    "        if match:\n",
    "            real_match_idx = start_idx + match.start()\n",
    "            if real_match_idx < end_idx:\n",
    "                end_idx = real_match_idx\n",
    "\n",
    "    return text[start_idx:end_idx].strip()\n",
    "\n",
    "# ==========================================\n",
    "# 4. AI LABELING FUNCTION\n",
    "# ==========================================\n",
    "def get_ai_label_batch(texts):\n",
    "    \"\"\"\n",
    "    Runs classification on a list of texts (Batch Processing)\n",
    "    \"\"\"\n",
    "    results = classifier(texts, CANDIDATE_LABELS, multi_label=False)\n",
    "    \n",
    "    final_labels = []\n",
    "    # Handle single result vs list of results\n",
    "    if isinstance(results, dict): results = [results]\n",
    "        \n",
    "    for res in results:\n",
    "        top_label = res['labels'][0]\n",
    "        final_labels.append(LABEL_MAP[top_label])\n",
    "        \n",
    "    return final_labels\n",
    "\n",
    "# ==========================================\n",
    "# 5. MAIN EXECUTION\n",
    "# ==========================================\n",
    "\n",
    "# ==========================================\n",
    "# 5. MAIN EXECUTION (OPTIMIZED)\n",
    "# ==========================================\n",
    "if __name__ == \"__main__\":\n",
    "    df = load_data()\n",
    "    \n",
    "    print(\"\\n>>> Step 2: Extracting Experience Sections...\")\n",
    "    df['extracted_text'] = df['text'].apply(extract_experience)\n",
    "    \n",
    "    # Filter valid rows & clean up\n",
    "    df_clean = df.dropna(subset=['extracted_text']).copy()\n",
    "    df_clean = df_clean[df_clean['extracted_text'].str.len() > 50]\n",
    "    \n",
    "    # Optional: Reset index to prevent issues with Dataset conversion\n",
    "    df_clean = df_clean.reset_index(drop=True)\n",
    "\n",
    "    print(f\"\\n>>> Step 3: AI Labeling ({len(df_clean)} samples) using Optimized Dataset Pipeline...\")\n",
    "\n",
    "    # 1. Convert Pandas DF to Hugging Face Dataset\n",
    "    hf_dataset = Dataset.from_pandas(df_clean[['extracted_text']])\n",
    "\n",
    "    # 2. Run Pipeline efficiently\n",
    "    # KeyDataset tells the pipeline to look at the \"extracted_text\" column\n",
    "    # batch_size=16 works well for 16GB VRAM. Reduce to 8 if you get OOM errors.\n",
    "    results = classifier(\n",
    "        KeyDataset(hf_dataset, \"extracted_text\"),\n",
    "        candidate_labels=CANDIDATE_LABELS,\n",
    "        multi_label=False,\n",
    "        batch_size=16, \n",
    "        truncation=True  # Automatically handles texts longer than 1024 tokens\n",
    "    )\n",
    "\n",
    "    # 3. Collect Results\n",
    "    ai_labels = []\n",
    "    # iterating over 'results' automatically triggers the progress bar internally if supported,\n",
    "    # or we can wrap it in tqdm\n",
    "    for res in tqdm(results, total=len(hf_dataset)):\n",
    "        top_label = res['labels'][0]\n",
    "        ai_labels.append(LABEL_MAP[top_label])\n",
    "\n",
    "    # 4. Save\n",
    "    df_clean['label'] = ai_labels\n",
    "    \n",
    "    print(\"\\n   - Final AI Distribution:\")\n",
    "    print(df_clean['label'].value_counts())\n",
    "    \n",
    "    df_training = df_clean[['extracted_text', 'label']].rename(columns={'extracted_text': 'text'})\n",
    "    filename = \"ai_labeled_experience_data.csv\"\n",
    "    df_training.to_csv(filename, index=False)\n",
    "    print(f\"\\n>>> DONE: Saved to '{filename}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "96734704",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading 'robust_experience_training_data.csv'...\n",
      "\n",
      "================================================================================\n",
      "  CATEGORY: FREELANCE\n",
      "================================================================================\n",
      "\n",
      "Sample #1:\n",
      "--------------------\n",
      "\"experience in IT industry and 3 years of experience in ServiceNow Platform Over 6 years of experience as a QA consultant and was responsible for testing efforts for implementation of all RMS change requests and supported multiple...\"\n",
      "--------------------\n",
      "\n",
      "Sample #2:\n",
      "--------------------\n",
      "\"Experience 01 1996 to Current Consultant Company Name City State Expanded new business opportunities in Texas for Program Management firm Developed contacts with Owners and Architects to develop relationships and solicit project possibilities Provided Project Management and Cost Consulting Services to Owners and Architects on major Higher...\"\n",
      "--------------------\n",
      "\n",
      "Sample #3:\n",
      "--------------------\n",
      "\"experience in IT industry and 3 years of experience in ServiceNow Platform Over 6 years of experience as a QA consultant and was responsible for testing efforts for implementation of all RMS change requests and supported multiple...\"\n",
      "--------------------\n",
      "\n",
      "================================================================================\n",
      "  CATEGORY: INTERNSHIP\n",
      "================================================================================\n",
      "\n",
      "Sample #1:\n",
      "--------------------\n",
      "\"Experience Fitness Instructor 02 2013 to Current Company Name City State Teach energetic workouts that are challenging and motivating yet safe Provide students individualized hands on adjustment throughout class Address each student by name during class and cultivate long term relationships Organized the 2014 Fitness Kick off Challenge in which local vendors provided educational seminars products ...\"\n",
      "--------------------\n",
      "\n",
      "Sample #2:\n",
      "--------------------\n",
      "\"experience and 6 months of Engineering Internship experience working on multi million dollar...\"\n",
      "--------------------\n",
      "\n",
      "Sample #3:\n",
      "--------------------\n",
      "\"Work Experience Intern Minnesota Pollution Control Agency Rochester MN June 2018 to March 2019 Create Maps in ArcGIS Monitor stream sites on the North Branch of the Whitewater River Collaborate with team members on various field work tasks including installation of longterm nitrate monitoring sites deployment of Sondes and HOBO temperature loggers and invertebrate sampling Graduate Assistant WSU R...\"\n",
      "--------------------\n",
      "\n",
      "================================================================================\n",
      "  CATEGORY: FULL-TIME\n",
      "================================================================================\n",
      "\n",
      "Sample #1:\n",
      "--------------------\n",
      "\"experience in the IT industry in support of national security objectives leveraging trained teams and experience in the innovation of protecting against cybersecurity threats and ensuring continuity of business operations Formal and specialized training in major areas of IT network cybersecurity and monitoring Excellent organization and communication...\"\n",
      "--------------------\n",
      "\n",
      "Sample #2:\n",
      "--------------------\n",
      "\"Work Experience IT SYSTEMS ENGINEER VENMILL INDUSTRIES INC February 2018 to Present Technical lead of IT Infrastructure Virtualization Networking Storage System administrator Database and computer security auditing Serve as IT technical advisor for Administration Operation day to day operations Prepare ITIS financial budgets and present project proposals to VP and executives of VenMill Inc Pointus...\"\n",
      "--------------------\n",
      "\n",
      "Sample #3:\n",
      "--------------------\n",
      "\"experience UI Front end development Authorized to work in the US for any employer Work Experience Front End Developer CARE USA Atlanta GA January 2016 to August 2017 Contract Developed various CARE web sites CARE Journeys Young Professionals SXD Scale Design Impact Magazine CARE Board Update CARE main web site Drupal content maintain web assets Worked on various web campaigns and initiatives worke...\"\n",
      "--------------------\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Check if the file from the previous step exists\n",
    "filename = \"robust_experience_training_data.csv\"\n",
    "\n",
    "if os.path.exists(filename):\n",
    "    print(f\"Loading '{filename}'...\")\n",
    "    df = pd.read_csv(filename)\n",
    "    \n",
    "    # --- VISUALIZATION FUNCTION ---\n",
    "    def view_samples(category):\n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(f\"  CATEGORY: {category.upper()}\")\n",
    "        print(\"=\"*80)\n",
    "        \n",
    "        # Get random samples for this category\n",
    "        subset = df[df['label'] == category]\n",
    "        \n",
    "        if subset.empty:\n",
    "            print(\"  (No samples found)\")\n",
    "            return\n",
    "\n",
    "        samples = subset.sample(n=min(3, len(subset)), random_state=42)\n",
    "        \n",
    "        for i, row in enumerate(samples.itertuples()):\n",
    "            print(f\"\\nSample #{i+1}:\")\n",
    "            print(\"-\" * 20)\n",
    "            \n",
    "            # Show the first 400 characters of the extracted text\n",
    "            text_preview = str(row.text)[:400].replace('\\n', ' ')\n",
    "            print(f\"\\\"{text_preview}...\\\"\")\n",
    "            print(\"-\" * 20)\n",
    "\n",
    "    # --- RUN FOR EACH CLASS ---\n",
    "    view_samples('Freelance')\n",
    "    view_samples('Internship')\n",
    "    view_samples('Full-time')\n",
    "\n",
    "else:\n",
    "    print(f\"Error: '{filename}' not found.\")\n",
    "    print(\"Please run the 'Final Data Preparation' script (previous step) first to generate the file.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17cdfbcb",
   "metadata": {},
   "source": [
    "suppose for data balancing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9d5d75f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Loading T5 Generative Model on CUDA...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f270f0a008844973a0c169a439a692d8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/892M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread Thread-4:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Python311\\Lib\\threading.py\", line 1038, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"c:\\Python311\\Lib\\site-packages\\tqdm\\_monitor.py\", line 84, in run\n",
      "    instance.refresh(nolock=True)\n",
      "  File \"c:\\Python311\\Lib\\site-packages\\tqdm\\std.py\", line 1347, in refresh\n",
      "    self.display()\n",
      "  File \"c:\\Python311\\Lib\\site-packages\\tqdm\\notebook.py\", line 171, in display\n",
      "    rtext.value = right\n",
      "    ^^^^^^^^^^^\n",
      "  File \"C:\\Users\\charls\\AppData\\Roaming\\Python\\Python311\\site-packages\\traitlets\\traitlets.py\", line 716, in __set__\n",
      "    self.set(obj, value)\n",
      "  File \"C:\\Users\\charls\\AppData\\Roaming\\Python\\Python311\\site-packages\\traitlets\\traitlets.py\", line 706, in set\n",
      "    obj._notify_trait(self.name, old_value, new_value)\n",
      "  File \"C:\\Users\\charls\\AppData\\Roaming\\Python\\Python311\\site-packages\\traitlets\\traitlets.py\", line 1513, in _notify_trait\n",
      "    self.notify_change(\n",
      "  File \"C:\\Users\\charls\\AppData\\Roaming\\Python\\Python311\\site-packages\\ipywidgets\\widgets\\widget.py\", line 700, in notify_change\n",
      "    self.send_state(key=name)\n",
      "  File \"C:\\Users\\charls\\AppData\\Roaming\\Python\\Python311\\site-packages\\ipywidgets\\widgets\\widget.py\", line 586, in send_state\n",
      "    self._send(msg, buffers=buffers)\n",
      "  File \"C:\\Users\\charls\\AppData\\Roaming\\Python\\Python311\\site-packages\\ipywidgets\\widgets\\widget.py\", line 825, in _send\n",
      "    self.comm.send(data=msg, buffers=buffers)\n",
      "  File \"C:\\Users\\charls\\AppData\\Roaming\\Python\\Python311\\site-packages\\comm\\base_comm.py\", line 147, in send\n",
      "    self.publish_msg(\n",
      "  File \"C:\\Users\\charls\\AppData\\Roaming\\Python\\Python311\\site-packages\\ipykernel\\comm\\comm.py\", line 42, in publish_msg\n",
      "    parent=self.kernel.get_parent(),\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\charls\\AppData\\Roaming\\Python\\Python311\\site-packages\\ipykernel\\kernelbase.py\", line 797, in get_parent\n",
      "    return self._shell_parent.get()\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "LookupError: <ContextVar name='shell_parent' at 0x0000017A37F71B20>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ö° Generative Augmentation for Full-time: Generating 1 new samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1 [00:00<?, ?it/s]C:\\Users\\charls\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\generation\\configuration_utils.py:676: UserWarning: `num_beams` is set to 1. However, `early_stopping` is set to `True` -- this flag is only used in beam-based generation modes. You should set `num_beams>1` or unset `early_stopping`.\n",
      "  warnings.warn(\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:05<00:00,  5.32s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ö° Generative Augmentation for Freelance: Generating 1962 new samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1962/1962 [1:44:23<00:00,  3.19s/it]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ö° Generative Augmentation for Internship: Generating 2000 new samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2000/2000 [35:00<00:00,  1.05s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ Final Generative Distribution:\n",
      "label\n",
      "Full-time     2114\n",
      "Internship    2114\n",
      "Freelance     2114\n",
      "Name: count, dtype: int64\n",
      "üíæ Saved high-quality data to generative_balanced_data.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "# ==========================================\n",
    "# 1. CONFIGURATION\n",
    "# ==========================================\n",
    "INPUT_FILE = \"ai_labeled_experience_data.csv\"\n",
    "OUTPUT_FILE = \"generative_balanced_data.csv\"\n",
    "TARGET_COUNT = 2114  # Target size for minority classes\n",
    "\n",
    "# We use a T5 model fine-tuned specifically for Paraphrasing\n",
    "# This is much safer than raw GPT generation because it stays grounded in your text\n",
    "MODEL_NAME = \"Vamsi/T5_Paraphrase_Paws\" \n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# ==========================================\n",
    "# 2. INITIALIZE GENERATIVE MODEL\n",
    "# ==========================================\n",
    "print(f\"üöÄ Loading T5 Generative Model on {device.upper()}...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(MODEL_NAME).to(device)\n",
    "\n",
    "def generate_paraphrase(text, num_return_sequences=1):\n",
    "    \"\"\"\n",
    "    Uses T5 to rewrite the resume text completely.\n",
    "    \"\"\"\n",
    "    # T5 requires the prefix \"paraphrase: \" to know what task to do\n",
    "    text = \"paraphrase: \" + text + \" </s>\"\n",
    "\n",
    "    encoding = tokenizer.encode_plus(\n",
    "        text, \n",
    "        padding=\"longest\", \n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    \n",
    "    input_ids = encoding[\"input_ids\"].to(device)\n",
    "    attention_masks = encoding[\"attention_mask\"].to(device)\n",
    "\n",
    "    # Generate\n",
    "    outputs = model.generate(\n",
    "        input_ids=input_ids, \n",
    "        attention_mask=attention_masks,\n",
    "        max_length=512,\n",
    "        do_sample=True, # Creativity enabled\n",
    "        top_k=120,\n",
    "        top_p=0.95,\n",
    "        early_stopping=True,\n",
    "        num_return_sequences=num_return_sequences\n",
    "    )\n",
    "\n",
    "    results = []\n",
    "    for output in outputs:\n",
    "        line = tokenizer.decode(output, skip_special_tokens=True, clean_up_tokenization_spaces=True)\n",
    "        results.append(line)\n",
    "        \n",
    "    return results\n",
    "\n",
    "# ==========================================\n",
    "# 3. AUGMENTATION LOOP\n",
    "# ==========================================\n",
    "def augment_class_generative(df, label, target_count):\n",
    "    existing_data = df[df['label'] == label]\n",
    "    current_count = len(existing_data)\n",
    "    \n",
    "    if current_count >= target_count:\n",
    "        print(f\"‚úÖ {label} is sufficient. Trimming.\")\n",
    "        return existing_data.sample(target_count, random_state=42)\n",
    "    \n",
    "    needed = target_count - current_count\n",
    "    print(f\"‚ö° Generative Augmentation for {label}: Generating {needed} new samples...\")\n",
    "    \n",
    "    texts = existing_data['text'].tolist()\n",
    "    new_rows = []\n",
    "    \n",
    "    # We loop until we have enough data\n",
    "    pbar = tqdm(total=needed)\n",
    "    while len(new_rows) < needed:\n",
    "        # Pick a random text\n",
    "        original = np.random.choice(texts)\n",
    "        \n",
    "        # Don't try to paraphrase huge blocks, T5 works best on sentences/paragraphs\n",
    "        # We truncate to ~500 chars for speed and accuracy\n",
    "        if len(original) > 500: original = original[:500]\n",
    "        \n",
    "        try:\n",
    "            # Generate a new variation\n",
    "            new_text = generate_paraphrase(original, num_return_sequences=1)[0]\n",
    "            \n",
    "            # Basic check to ensure it's not identical\n",
    "            if new_text.lower() != original.lower():\n",
    "                new_rows.append({'text': new_text, 'label': label})\n",
    "                pbar.update(1)\n",
    "        except:\n",
    "            continue\n",
    "            \n",
    "    pbar.close()\n",
    "    \n",
    "    synthetic_df = pd.DataFrame(new_rows)\n",
    "    combined_df = pd.concat([existing_data, synthetic_df], ignore_index=True)\n",
    "    \n",
    "    return combined_df.iloc[:target_count]\n",
    "\n",
    "# ==========================================\n",
    "# 4. MAIN EXECUTION\n",
    "# ==========================================\n",
    "if __name__ == \"__main__\":\n",
    "    df = pd.read_csv(INPUT_FILE)\n",
    "    \n",
    "    dfs = []\n",
    "    # 1. Full-time (Keep as is/Trim)\n",
    "    dfs.append(augment_class_generative(df, \"Full-time\", TARGET_COUNT))\n",
    "    \n",
    "    # 2. Freelance (Generate)\n",
    "    dfs.append(augment_class_generative(df, \"Freelance\", TARGET_COUNT))\n",
    "    \n",
    "    # 3. Internship (Generate)\n",
    "    dfs.append(augment_class_generative(df, \"Internship\", TARGET_COUNT))\n",
    "    \n",
    "    final_df = pd.concat(dfs).sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "    \n",
    "    print(\"\\n‚úÖ Final Generative Distribution:\")\n",
    "    print(final_df['label'].value_counts())\n",
    "    \n",
    "    final_df.to_csv(OUTPUT_FILE, index=False)\n",
    "    print(f\"üíæ Saved high-quality data to {OUTPUT_FILE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b9e9ae3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚è≥ Loading balanced dataset...\n",
      "‚úÖ Training Samples: 5073\n",
      "‚úÖ Validation Samples: 1269\n",
      "‚è≥ Tokenizing data...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b6043146dee24883bb3f07e042b0fcc7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/5073 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a811d40c69cf437795c7b83cc657e278",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1269 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Initializing Model: distilbert-base-uncased\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\charls\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ü§ó Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "C:\\Users\\charls\\AppData\\Local\\Temp\\ipykernel_9048\\1726729537.py:121: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üî• STARTING TRAINING...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c176f6ca6a348d2892aeb64ade96f5c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/954 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "45b730eefbb54454a46221f5f8db454b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/80 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.2584383487701416, 'eval_accuracy': 0.91725768321513, 'eval_runtime': 4.8488, 'eval_samples_per_second': 261.713, 'eval_steps_per_second': 16.499, 'epoch': 1.0}\n",
      "{'loss': 0.4141, 'grad_norm': 1.7056277990341187, 'learning_rate': 9.517819706498952e-06, 'epoch': 1.57}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "054866bbce6e4866824308a71f746515",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/80 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.12816651165485382, 'eval_accuracy': 0.9613869188337274, 'eval_runtime': 5.4047, 'eval_samples_per_second': 234.797, 'eval_steps_per_second': 14.802, 'epoch': 2.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7e385b34feb345ba9753445e93307b6b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/80 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.11577248573303223, 'eval_accuracy': 0.9676910953506698, 'eval_runtime': 5.3835, 'eval_samples_per_second': 235.722, 'eval_steps_per_second': 14.86, 'epoch': 3.0}\n",
      "{'train_runtime': 221.2839, 'train_samples_per_second': 68.776, 'train_steps_per_second': 4.311, 'train_loss': 0.27179700023723097, 'epoch': 3.0}\n",
      "\n",
      "üìä FINAL EVALUATION:\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "144163fe49c846659ed6a75f13442f99",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/80 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9677\n",
      "\n",
      "üíæ Saving model to ./saved_bert_model_final...\n",
      "‚úÖ Training Complete! You can now use this model in your ranking pipeline.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from transformers import (\n",
    "    AutoTokenizer, \n",
    "    AutoModelForSequenceClassification, \n",
    "    Trainer, \n",
    "    TrainingArguments, \n",
    "    DataCollatorWithPadding\n",
    ")\n",
    "from datasets import Dataset\n",
    "\n",
    "# ==========================================\n",
    "# 1. CONFIGURATION\n",
    "# ==========================================\n",
    "INPUT_FILE = \"generative_balanced_data.csv\"\n",
    "OUTPUT_DIR = \"./saved_bert_model_final\"\n",
    "MODEL_CHECKPOINT = \"distilbert-base-uncased\"  # Fast & Accurate. Use \"bert-base-uncased\" for max accuracy.\n",
    "BATCH_SIZE = 16\n",
    "EPOCHS = 3\n",
    "LEARNING_RATE = 2e-5\n",
    "MAX_LEN = 128\n",
    "\n",
    "# Label Mapping (Must match your classes exactly)\n",
    "id2label = {0: \"Freelance\", 1: \"Full-time\", 2: \"Internship\"}\n",
    "label2id = {\"Freelance\": 0, \"Full-time\": 1, \"Internship\": 2}\n",
    "\n",
    "# ==========================================\n",
    "# 2. DATA PREPARATION\n",
    "# ==========================================\n",
    "def load_and_prepare_data():\n",
    "    print(\"‚è≥ Loading balanced dataset...\")\n",
    "    df = pd.read_csv(INPUT_FILE)\n",
    "    \n",
    "    # Shuffle\n",
    "    df = df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "    \n",
    "    # Map text labels to integers\n",
    "    df['label_id'] = df['label'].map(label2id)\n",
    "    \n",
    "    # Split Train/Test (80/20)\n",
    "    train_df, val_df = train_test_split(df, test_size=0.2, random_state=42, stratify=df['label_id'])\n",
    "    \n",
    "    print(f\"‚úÖ Training Samples: {len(train_df)}\")\n",
    "    print(f\"‚úÖ Validation Samples: {len(val_df)}\")\n",
    "    \n",
    "    # Convert to Hugging Face Datasets\n",
    "    train_ds = Dataset.from_pandas(train_df[['text', 'label_id']])\n",
    "    val_ds = Dataset.from_pandas(val_df[['text', 'label_id']])\n",
    "    \n",
    "    return train_ds, val_ds\n",
    "\n",
    "# ==========================================\n",
    "# 3. TOKENIZATION\n",
    "# ==========================================\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_CHECKPOINT)\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    # Truncation is vital for BERT\n",
    "    return tokenizer(examples[\"text\"], truncation=True, padding=False, max_length=MAX_LEN)\n",
    "\n",
    "# ==========================================\n",
    "# 4. METRICS FUNCTION\n",
    "# ==========================================\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    acc = accuracy_score(labels, predictions)\n",
    "    return {\"accuracy\": acc}\n",
    "\n",
    "# ==========================================\n",
    "# 5. MAIN TRAINING PIPELINE\n",
    "# ==========================================\n",
    "if __name__ == \"__main__\":\n",
    "    # A. Load Data\n",
    "    train_dataset, val_dataset = load_and_prepare_data()\n",
    "\n",
    "    # B. Tokenize\n",
    "    print(\"‚è≥ Tokenizing data...\")\n",
    "    tokenized_train = train_dataset.map(preprocess_function, batched=True)\n",
    "    tokenized_val = val_dataset.map(preprocess_function, batched=True)\n",
    "    \n",
    "    # Rename 'label_id' to 'labels' (Required by HF Trainer)\n",
    "    tokenized_train = tokenized_train.rename_column(\"label_id\", \"labels\")\n",
    "    tokenized_val = tokenized_val.rename_column(\"label_id\", \"labels\")\n",
    "    \n",
    "    # Remove unnecessary columns\n",
    "    tokenized_train = tokenized_train.remove_columns([\"text\", \"__index_level_0__\"])\n",
    "    tokenized_val = tokenized_val.remove_columns([\"text\", \"__index_level_0__\"])\n",
    "\n",
    "    # C. Data Collator (Dynamic Padding)\n",
    "    data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "    # D. Initialize Model\n",
    "    print(f\"üöÄ Initializing Model: {MODEL_CHECKPOINT}\")\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(\n",
    "        MODEL_CHECKPOINT, \n",
    "        num_labels=3,\n",
    "        id2label=id2label,\n",
    "        label2id=label2id\n",
    "    )\n",
    "\n",
    "    # E. Training Arguments\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=\"./results\",\n",
    "        learning_rate=LEARNING_RATE,\n",
    "        per_device_train_batch_size=BATCH_SIZE,\n",
    "        per_device_eval_batch_size=BATCH_SIZE,\n",
    "        num_train_epochs=EPOCHS,\n",
    "        weight_decay=0.01,\n",
    "        evaluation_strategy=\"epoch\",\n",
    "        save_strategy=\"epoch\",\n",
    "        load_best_model_at_end=True,\n",
    "        push_to_hub=False,\n",
    "        report_to=\"none\" # Disable WandB logging\n",
    "    )\n",
    "\n",
    "    # F. Trainer\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=tokenized_train,\n",
    "        eval_dataset=tokenized_val,\n",
    "        tokenizer=tokenizer,\n",
    "        data_collator=data_collator,\n",
    "        compute_metrics=compute_metrics,\n",
    "    )\n",
    "\n",
    "    # G. TRAIN!\n",
    "    print(\"\\nüî• STARTING TRAINING...\")\n",
    "    trainer.train()\n",
    "\n",
    "    # H. Evaluate\n",
    "    print(\"\\nüìä FINAL EVALUATION:\")\n",
    "    eval_results = trainer.evaluate()\n",
    "    print(f\"Accuracy: {eval_results['eval_accuracy']:.4f}\")\n",
    "\n",
    "    # I. Save Final Model\n",
    "    print(f\"\\nüíæ Saving model to {OUTPUT_DIR}...\")\n",
    "    trainer.save_model(OUTPUT_DIR)\n",
    "    \n",
    "    # Also save the classes.npy for your inference script\n",
    "    np.save(f\"{OUTPUT_DIR}/classes.npy\", np.array([\"Freelance\", \"Full-time\", \"Internship\"]))\n",
    "    print(\"‚úÖ Training Complete! You can now use this model in your ranking pipeline.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97532e73",
   "metadata": {},
   "source": [
    "full training mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5869f5cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚è≥ Loading model from ./saved_bert_model_final on cuda...\n",
      "‚úÖ Model loaded successfully!\n",
      "\n",
      "TEXT SAMPLE                                                  | PREDICTION   | CONFIDENCE\n",
      "------------------------------------------------------------------------------------------\n",
      "fulltime Software Engineer at Google leading a team of 10... | Freelance    | 99.4% ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "Staff Accountant managing month-end close and general led... | Freelance    | 99.4% ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "Built a custom WordPress site for a local bakery as a one... | Freelance    | 99.5% ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "Self-employed graphic designer working with various clien... | Freelance    | 99.4% ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "Summer Intern assisting the marketing team with social me... | Internship   | 94.8% ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "Engineering Student Trainee shadowing senior developers....  | Internship   | 99.4% ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "Contract role to fix specific bugs in the payment gateway... | Freelance    | 98.9% ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "Founder of a small startup building an iOS app....           | Freelance    | 99.3% ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import os\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# ==========================================\n",
    "# 1. CONFIGURATION\n",
    "# ==========================================\n",
    "MODEL_PATH = \"./saved_bert_model_final\"\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "class ExperienceClassifier:\n",
    "    def __init__(self, model_dir):\n",
    "        print(f\"‚è≥ Loading model from {model_dir} on {DEVICE}...\")\n",
    "        \n",
    "        # Load Model & Tokenizer\n",
    "        try:\n",
    "            self.tokenizer = AutoTokenizer.from_pretrained(model_dir)\n",
    "            self.model = AutoModelForSequenceClassification.from_pretrained(model_dir)\n",
    "            self.model.to(DEVICE)\n",
    "            self.model.eval() # Set to evaluation mode\n",
    "            \n",
    "            # Load Class Labels (Saved during training)\n",
    "            classes_file = os.path.join(model_dir, \"classes.npy\")\n",
    "            if os.path.exists(classes_file):\n",
    "                self.labels = np.load(classes_file, allow_pickle=True)\n",
    "            else:\n",
    "                print(\"‚ö†Ô∏è Warning: classes.npy not found. Using default labels.\")\n",
    "                self.labels = [\"Freelance\", \"Full-time\", \"Internship\"]\n",
    "                \n",
    "            print(\"‚úÖ Model loaded successfully!\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error loading model: {e}\")\n",
    "            self.model = None\n",
    "\n",
    "    def predict(self, text):\n",
    "        if not self.model: return \"Error\", 0.0\n",
    "\n",
    "        # Tokenize\n",
    "        inputs = self.tokenizer(\n",
    "            text, \n",
    "            return_tensors=\"pt\", \n",
    "            truncation=True, \n",
    "            padding=True, \n",
    "            max_length=128\n",
    "        ).to(DEVICE)\n",
    "\n",
    "        # Inference\n",
    "        with torch.no_grad():\n",
    "            logits = self.model(**inputs).logits\n",
    "            probs = F.softmax(logits, dim=1)\n",
    "            \n",
    "        # Get Top Prediction\n",
    "        conf, idx = torch.max(probs, dim=1)\n",
    "        return self.labels[idx.item()], conf.item()\n",
    "\n",
    "# ==========================================\n",
    "# 2. RUN TESTS\n",
    "# ==========================================\n",
    "if __name__ == \"__main__\":\n",
    "    # Initialize Classifier\n",
    "    classifier = ExperienceClassifier(MODEL_PATH)\n",
    "\n",
    "    # Test Cases (Mix of clear and tricky examples)\n",
    "    test_cases = [\n",
    "        # Full-time examples\n",
    "        \"fulltime Software Engineer at Google leading a team of 10 developers.\",\n",
    "        \"Staff Accountant managing month-end close and general ledger.\",\n",
    "        \n",
    "        # Freelance examples\n",
    "        \"Built a custom WordPress site for a local bakery as a one-off project.\",\n",
    "        \"Self-employed graphic designer working with various clients on Upwork.\",\n",
    "        \n",
    "        # Internship examples\n",
    "        \"Summer Intern assisting the marketing team with social media campaigns.\",\n",
    "        \"Engineering Student Trainee shadowing senior developers.\",\n",
    "        \n",
    "        # Tricky / Ambiguous examples\n",
    "        \"Contract role to fix specific bugs in the payment gateway (3 months).\",\n",
    "        \"Founder of a small startup building an iOS app.\"\n",
    "    ]\n",
    "\n",
    "    print(f\"\\n{'TEXT SAMPLE':<60} | {'PREDICTION':<12} | {'CONFIDENCE'}\")\n",
    "    print(\"-\" * 90)\n",
    "\n",
    "    for text in test_cases:\n",
    "        label, score = classifier.predict(text)\n",
    "        \n",
    "        # Visual confidence bar\n",
    "        bar = \"‚ñà\" * int(score * 10)\n",
    "        \n",
    "        print(f\"{text[:57] + '...':<60} | {label:<12} | {score:.1%} {bar}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "d9566b92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Loading Data & Splitting...\n",
      "Classes: ['Freelance' 'Full-time' 'Internship']\n",
      "Training Samples (Raw): 26630\n",
      "Test Samples (Pure): 6658\n",
      "\n",
      "2. Augmenting Training Data (BERT)...\n",
      "   Augmenting Freelance: Creating 2668 new samples...\n",
      "   Augmenting Internship: Creating 2794 new samples...\n",
      "Final Training Set Size: 9000\n",
      "label\n",
      "Internship    3000\n",
      "Freelance     3000\n",
      "Full-time     3000\n",
      "Name: count, dtype: int64\n",
      "\n",
      "3. Training BERT...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\charls\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 563/563 [04:06<00:00,  2.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Avg Loss: 0.35194173762591335\n",
      "Epoch 2/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 563/563 [03:58<00:00,  2.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Avg Loss: 0.10765275600172751\n",
      "Epoch 3/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 563/563 [04:01<00:00,  2.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Avg Loss: 0.0458391964432269\n",
      "Epoch 4/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 563/563 [04:00<00:00,  2.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Avg Loss: 0.025387587201315082\n",
      "\n",
      "4. Final Evaluation (On Pure Unseen Data)...\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   Freelance       0.30      0.78      0.43        83\n",
      "   Full-time       1.00      0.96      0.98      6523\n",
      "  Internship       0.29      0.88      0.44        52\n",
      "\n",
      "    accuracy                           0.96      6658\n",
      "   macro avg       0.53      0.88      0.62      6658\n",
      "weighted avg       0.98      0.96      0.97      6658\n",
      "\n",
      "Model saved.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import nlpaug.augmenter.word as naw\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, AdamW\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.utils import resample\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "# --- CONFIGURATION ---\n",
    "INPUT_FILE = \"robust_experience_training_data_unbalanced.csv\" # USE THE ORIGINAL UNBALANCED FILE\n",
    "SAVE_PATH = \"./saved_bert_model_v2\"\n",
    "MAX_LEN = 128\n",
    "BATCH_SIZE = 16\n",
    "EPOCHS = 4\n",
    "TARGET_PER_CLASS = 3000 # Augment up to this number\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# --- 1. LOAD & SPLIT FIRST (CRITICAL STEP) ---\n",
    "if not os.path.exists(INPUT_FILE):\n",
    "    raise FileNotFoundError(\"Please provide the original 'unbalanced' csv file.\")\n",
    "\n",
    "print(\"1. Loading Data & Splitting...\")\n",
    "df = pd.read_csv(INPUT_FILE)\n",
    "\n",
    "# Encode Labels\n",
    "le = LabelEncoder()\n",
    "df['label_encoded'] = le.fit_transform(df['label'])\n",
    "classes = le.classes_\n",
    "print(f\"Classes: {classes}\")\n",
    "\n",
    "# SPLIT FIRST! Keep 20% pure for testing.\n",
    "X_train_raw, X_test, y_train_raw, y_test = train_test_split(\n",
    "    df['text'].values, \n",
    "    df['label_encoded'].values, \n",
    "    test_size=0.2, \n",
    "    random_state=42, \n",
    "    stratify=df['label_encoded']\n",
    ")\n",
    "\n",
    "# Reassemble Training Set for Augmentation\n",
    "train_df = pd.DataFrame({'text': X_train_raw, 'label_encoded': y_train_raw})\n",
    "train_df['label'] = le.inverse_transform(train_df['label_encoded'])\n",
    "\n",
    "print(f\"Training Samples (Raw): {len(train_df)}\")\n",
    "print(f\"Test Samples (Pure): {len(X_test)}\")\n",
    "\n",
    "# --- 2. AUGMENT ONLY TRAINING DATA ---\n",
    "print(\"\\n2. Augmenting Training Data (BERT)...\")\n",
    "aug = naw.ContextualWordEmbsAug(model_path='bert-base-uncased', action=\"substitute\", device='cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "augmented_dfs = []\n",
    "\n",
    "for class_name in classes:\n",
    "    # Get all samples for this class\n",
    "    subset = train_df[train_df['label'] == class_name]\n",
    "    \n",
    "    # If Majority Class (Full-time), Downsample\n",
    "    if len(subset) > TARGET_PER_CLASS:\n",
    "        subset = resample(subset, replace=False, n_samples=TARGET_PER_CLASS, random_state=42)\n",
    "        augmented_dfs.append(subset)\n",
    "    \n",
    "    # If Minority Class, Augment\n",
    "    else:\n",
    "        # Add original samples first\n",
    "        augmented_dfs.append(subset)\n",
    "        \n",
    "        needed = TARGET_PER_CLASS - len(subset)\n",
    "        print(f\"   Augmenting {class_name}: Creating {needed} new samples...\")\n",
    "        \n",
    "        new_texts = []\n",
    "        original_texts = subset['text'].tolist()\n",
    "        \n",
    "        # Cycle through originals to create new ones\n",
    "        while len(new_texts) < needed:\n",
    "            for text in original_texts:\n",
    "                if len(new_texts) >= needed: break\n",
    "                try:\n",
    "                    # Augment\n",
    "                    aug_text = aug.augment(str(text))\n",
    "                    if isinstance(aug_text, list): aug_text = aug_text[0]\n",
    "                    if aug_text != text:\n",
    "                        new_texts.append(aug_text)\n",
    "                except:\n",
    "                    continue\n",
    "        \n",
    "        # Add new synthetic samples\n",
    "        temp_df = pd.DataFrame({'text': new_texts})\n",
    "        temp_df['label'] = class_name\n",
    "        temp_df['label_encoded'] = le.transform([class_name])[0]\n",
    "        augmented_dfs.append(temp_df)\n",
    "\n",
    "# Combine\n",
    "df_train_final = pd.concat(augmented_dfs).sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "print(f\"Final Training Set Size: {len(df_train_final)}\")\n",
    "print(df_train_final['label'].value_counts())\n",
    "\n",
    "# --- 3. PREPARE DATASETS ---\n",
    "class ResumeDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_len):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        text = str(self.texts[item])\n",
    "        label = self.labels[item]\n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            text, add_special_tokens=True, max_length=self.max_len,\n",
    "            return_token_type_ids=False, padding='max_length',\n",
    "            truncation=True, return_attention_mask=True, return_tensors='pt',\n",
    "        )\n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'labels': torch.tensor(label, dtype=torch.long)\n",
    "        }\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "train_dataset = ResumeDataset(df_train_final['text'].values, df_train_final['label_encoded'].values, tokenizer, MAX_LEN)\n",
    "test_dataset = ResumeDataset(X_test, y_test, tokenizer, MAX_LEN)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE)\n",
    "\n",
    "# --- 4. TRAIN ---\n",
    "print(\"\\n3. Training BERT...\")\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=len(classes))\n",
    "model = model.to(DEVICE)\n",
    "optimizer = AdamW(model.parameters(), lr=2e-5)\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    print(f\"Epoch {epoch+1}/{EPOCHS}\")\n",
    "    \n",
    "    for batch in tqdm(train_loader):\n",
    "        input_ids = batch['input_ids'].to(DEVICE)\n",
    "        mask = batch['attention_mask'].to(DEVICE)\n",
    "        labels = batch['labels'].to(DEVICE)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(input_ids, attention_mask=mask, labels=labels)\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "        \n",
    "    print(f\"   Avg Loss: {total_loss / len(train_loader)}\")\n",
    "\n",
    "# --- 5. EVALUATE ---\n",
    "print(\"\\n4. Final Evaluation (On Pure Unseen Data)...\")\n",
    "model.eval()\n",
    "preds, true_labels = [], []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in test_loader:\n",
    "        input_ids = batch['input_ids'].to(DEVICE)\n",
    "        mask = batch['attention_mask'].to(DEVICE)\n",
    "        outputs = model(input_ids, attention_mask=mask)\n",
    "        _, prediction = torch.max(outputs.logits, dim=1)\n",
    "        preds.extend(prediction.cpu().numpy())\n",
    "        true_labels.extend(batch['labels'].numpy())\n",
    "\n",
    "print(classification_report(true_labels, preds, target_names=classes))\n",
    "\n",
    "# Save\n",
    "if not os.path.exists(SAVE_PATH): os.makedirs(SAVE_PATH)\n",
    "model.save_pretrained(SAVE_PATH)\n",
    "tokenizer.save_pretrained(SAVE_PATH)\n",
    "np.save(os.path.join(SAVE_PATH, 'classes.npy'), classes)\n",
    "print(\"Model saved.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ac501db",
   "metadata": {},
   "source": [
    "this part it will prepare for testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cd26c0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading BERT model from ./saved_bert_model...\n",
      "‚úÖ Model Loaded Successfully!\n",
      "Using Device: cuda\n",
      "Classes: ['Freelance' 'Full-time' 'Internship']\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "import os\n",
    "\n",
    "# --- CONFIGURATION ---\n",
    "MODEL_PATH = \"./saved_bert_model_v2\"\n",
    "MAX_LEN = 128\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# --- LOAD MODEL & TOKENIZER ---\n",
    "if not os.path.exists(MODEL_PATH):\n",
    "    print(f\"Error: Model not found at {MODEL_PATH}. Did you run the training cell?\")\n",
    "else:\n",
    "    print(f\"Loading BERT model from {MODEL_PATH}...\")\n",
    "    \n",
    "    # Load architecture and weights\n",
    "    model = BertForSequenceClassification.from_pretrained(MODEL_PATH)\n",
    "    model = model.to(device)\n",
    "    model.eval() # Freeze for inference\n",
    "\n",
    "    # Load tokenizer\n",
    "    tokenizer = BertTokenizer.from_pretrained(MODEL_PATH)\n",
    "\n",
    "    # Load class names\n",
    "    classes = np.load(os.path.join(MODEL_PATH, 'classes.npy'), allow_pickle=True)\n",
    "    \n",
    "    print(f\"‚úÖ Model Loaded Successfully!\")\n",
    "    print(f\"Using Device: {device}\")\n",
    "    print(f\"Classes: {classes}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b3fe0d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_resume(text):\n",
    "    \"\"\"\n",
    "    Accepts a resume string and returns the predicted label and confidence score.\n",
    "    \"\"\"\n",
    "    if not text: return \"Empty\", 0.0\n",
    "\n",
    "    # 1. Tokenize\n",
    "    encoding = tokenizer.encode_plus(\n",
    "        text,\n",
    "        add_special_tokens=True,\n",
    "        max_length=MAX_LEN,\n",
    "        return_token_type_ids=False,\n",
    "        padding='max_length',\n",
    "        truncation=True,\n",
    "        return_attention_mask=True,\n",
    "        return_tensors='pt',\n",
    "    )\n",
    "\n",
    "    # 2. Move to GPU/CPU\n",
    "    input_ids = encoding['input_ids'].to(device)\n",
    "    attention_mask = encoding['attention_mask'].to(device)\n",
    "\n",
    "    # 3. Predict\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        logits = outputs.logits\n",
    "        probs = torch.nn.functional.softmax(logits, dim=1)\n",
    "    \n",
    "    # 4. Decode Result\n",
    "    confidence, prediction_idx = torch.max(probs, dim=1)\n",
    "    predicted_label = classes[prediction_idx.item()]\n",
    "    confidence_score = confidence.item()\n",
    "\n",
    "    return predicted_label, confidence_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "39a50cc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PREDICTION      | CONF.    | RESUME TEXT\n",
      "==========================================================================================\n",
      "FULL-TIME       | 100.0% ‚úÖ | \"I spent my summer assisting the backend team with API docume...\"\n",
      "FULL-TIME       | 80.5% ‚ö†Ô∏è | \"Shadowed the Senior UX Designer and helped conduct user rese...\"\n",
      "INTERNSHIP      | 100.0% ‚úÖ | \"Part of the university co-op program, working on data entry ...\"\n",
      "FULL-TIME       | 100.0% ‚úÖ | \"I take on various graphic design projects on Upwork, managin...\"\n",
      "FULL-TIME       | 100.0% ‚úÖ | \"Served as the Lead Developer for 4 years, managing a team of...\"\n",
      "FULL-TIME       | 100.0% ‚úÖ | \"Responsible for the end-to-end deployment of the company's m...\"\n",
      "FULL-TIME       | 99.9% ‚úÖ | \"Employed since 2018 as a Senior Analyst, handling daily oper...\"\n",
      "FULL-TIME       | 90.2% ‚úÖ | \"I worked for 6 months covering a maternity leave, handling f...\"\n",
      "FULL-TIME       | 93.5% ‚úÖ | \"I built the entire mobile app by myself during the weekends ...\"\n",
      "FULL-TIME       | 100.0% ‚úÖ | \"Junior developer responsible for updating the UI, reporting ...\"\n"
     ]
    }
   ],
   "source": [
    "# --- LIST OF TEST CASES ---\n",
    "test_cases = [\n",
    "    # --- INTERNSHIP EXAMPLES ---\n",
    "    \"I spent my summer assisting the backend team with API documentation and minor bug fixes.\",\n",
    "    \"Shadowed the Senior UX Designer and helped conduct user research surveys for 3 months.\",\n",
    "    \"Part of the university co-op program, working on data entry and basic SQL queries.\",\n",
    "    \n",
    "    # --- FREELANCE EXAMPLES ---\n",
    "    \"I take on various graphic design projects on Upwork, managing my own schedule and clients.\",\n",
    "\n",
    "    \n",
    "    # --- FULL-TIME EXAMPLES ---\n",
    "    \"Served as the Lead Developer for 4 years, managing a team of 10 engineers.\",\n",
    "    \"Responsible for the end-to-end deployment of the company's main payment gateway.\",\n",
    "    \"Employed since 2018 as a Senior Analyst, handling daily operations and quarterly reporting.\",\n",
    "    \n",
    "    # --- TRICKY / AMBIGUOUS EXAMPLES (The real test!) ---\n",
    "    \"I worked for 6 months covering a maternity leave, handling full server access.\", \n",
    "    # ^ (Could be Full-time or Contract/Freelance)\n",
    "    \n",
    "    \"I built the entire mobile app by myself during the weekends while studying.\", \n",
    "    # ^ (Likely Freelance or Personal Project, shouldn't be Full-time)\n",
    "    \n",
    "    \"Junior developer responsible for updating the UI, reporting to the CTO.\" \n",
    "    # ^ (Context implies Full-time, but description is simple)\n",
    "]\n",
    "\n",
    "# --- RUN BATCH PREDICTION ---\n",
    "print(f\"{'PREDICTION':<15} | {'CONF.':<8} | {'RESUME TEXT'}\")\n",
    "print(\"=\"*90)\n",
    "\n",
    "for text in test_cases:\n",
    "    label, score = predict_resume(text)\n",
    "    \n",
    "    # Color coding for easier reading (optional)\n",
    "    # 90%+ confidence = High, <70% = Low\n",
    "    indicator = \"‚úÖ\" if score > 0.9 else \"‚ö†Ô∏è\"\n",
    "    \n",
    "    print(f\"{label.upper():<15} | {score*100:.1f}% {indicator} | \\\"{text[:60]}...\\\"\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
